//
//  AppleSTTService.swift
//  ios_realtime_trans
//
//  Apple å…§å»º STT æœå‹™ï¼ˆé›™èªè¨€ä¸¦è¡Œè­˜åˆ¥ï¼‰
//  ä½¿ç”¨ SFSpeechRecognizer é€²è¡Œè¨­å‚™ç«¯èªéŸ³è­˜åˆ¥
//  å„ªé»ï¼šå…è²»ã€é›¢ç·šå¯ç”¨ã€ä½å»¶é²ã€ç„¡ API é…é¡é™åˆ¶
//  ç¼ºé»ï¼šèªè¨€æ”¯æ´è¼ƒå°‘ã€éœ€è¦ç”¨æˆ¶ä¸‹è¼‰èªè¨€åŒ…
//

import Foundation
import Speech
import AVFoundation
import Combine

/// Apple STT æœå‹™ï¼ˆè¨­å‚™ç«¯é›™èªè¨€ä¸¦è¡Œè­˜åˆ¥ï¼‰
class AppleSTTService: NSObject, WebSocketServiceProtocol {

    // MARK: - Properties

    /// é›™èªè¨€è­˜åˆ¥å™¨
    private var sourceRecognizer: SFSpeechRecognizer?
    private var targetRecognizer: SFSpeechRecognizer?

    /// è­˜åˆ¥ä»»å‹™
    private var sourceTask: SFSpeechRecognitionTask?
    private var targetTask: SFSpeechRecognitionTask?

    /// è­˜åˆ¥è«‹æ±‚
    private var sourceRequest: SFSpeechAudioBufferRecognitionRequest?
    private var targetRequest: SFSpeechAudioBufferRecognitionRequest?

    /// ç•¶å‰èªè¨€è¨­ç½®
    private var sourceLang: Language = .zh
    private var targetLang: Language = .en

    /// é€£æ¥ç‹€æ…‹
    private(set) var connectionState: WebSocketConnectionState = .disconnected

    /// ç¿»è­¯æ¨¡å‹é¸æ“‡
    var translationProvider: TranslationProvider = .gemini

    /// ä¼ºæœå™¨ URLï¼ˆç”¨æ–¼ç¿»è­¯ APIï¼‰
    private var serverURL: String = ""

    // MARK: - Publishers

    private let _transcriptSubject = PassthroughSubject<TranscriptMessage, Never>()
    private let _translationSubject = PassthroughSubject<(String, String), Never>()
    private let _segmentedTranslationSubject = PassthroughSubject<(String, [TranslationSegment]), Never>()
    private let _correctionSubject = PassthroughSubject<(String, String), Never>()
    private let _errorSubject = PassthroughSubject<String, Never>()

    var transcriptPublisher: AnyPublisher<TranscriptMessage, Never> {
        _transcriptSubject.eraseToAnyPublisher()
    }
    var translationPublisher: AnyPublisher<(String, String), Never> {
        _translationSubject.eraseToAnyPublisher()
    }
    var segmentedTranslationPublisher: AnyPublisher<(String, [TranslationSegment]), Never> {
        _segmentedTranslationSubject.eraseToAnyPublisher()
    }
    var correctionPublisher: AnyPublisher<(String, String), Never> {
        _correctionSubject.eraseToAnyPublisher()
    }
    var errorPublisher: AnyPublisher<String, Never> {
        _errorSubject.eraseToAnyPublisher()
    }

    // MARK: - ä¿¡å¿ƒåº¦è¿½è¹¤

    /// æœ€æ–°çš„ä¾†æºèªè¨€è­˜åˆ¥çµæœ
    private var lastSourceResult: RecognitionResult?
    /// æœ€æ–°çš„ç›®æ¨™èªè¨€è­˜åˆ¥çµæœ
    private var lastTargetResult: RecognitionResult?

    /// è­˜åˆ¥çµæœçµæ§‹
    private struct RecognitionResult {
        let text: String
        let confidence: Float
        let language: String
        let isFinal: Bool
        let timestamp: Date
    }

    /// ä¸Šä¸€æ¬¡ç™¼é€çš„çµæœï¼ˆç”¨æ–¼å»é‡ï¼‰
    private var lastEmittedText: String = ""
    private var lastEmittedLanguage: String = ""

    // MARK: - é˜²æŠ–èˆ‡è¶…æ™‚

    /// é˜²æŠ–è¨ˆæ™‚å™¨ï¼ˆåˆä½µçŸ­æ™‚é–“å…§çš„çµæœï¼‰
    private var debounceTimer: Timer?
    private let debounceInterval: TimeInterval = 0.15  // 150ms é˜²æŠ–

    /// ä»»å‹™é‡å»ºè¨ˆæ™‚å™¨ï¼ˆApple STT æœ‰ç´„ 1 åˆ†é˜é™åˆ¶ï¼‰
    private var taskRebuildTimer: Timer?
    private let taskRebuildInterval: TimeInterval = 55.0  // 55 ç§’é‡å»º

    /// éŸ³é »æ ¼å¼
    private let audioFormat = AVAudioFormat(
        commonFormat: .pcmFormatFloat32,
        sampleRate: 16000,
        channels: 1,
        interleaved: false
    )!

    // MARK: - çµ±è¨ˆ

    private var recognitionStartTime: Date?
    private var totalAudioDuration: TimeInterval = 0

    // MARK: - Initialization

    override init() {
        super.init()
        print("âœ… [Apple STT] æœå‹™åˆå§‹åŒ–")
    }

    // MARK: - WebSocketServiceProtocol

    func connect(serverURL: String, sourceLang: Language, targetLang: Language) {
        self.serverURL = serverURL
        self.sourceLang = sourceLang
        self.targetLang = targetLang

        connectionState = .connecting

        // è«‹æ±‚èªéŸ³è­˜åˆ¥æ¬Šé™
        SFSpeechRecognizer.requestAuthorization { [weak self] status in
            DispatchQueue.main.async {
                self?.handleAuthorizationStatus(status)
            }
        }
    }

    func disconnect() {
        print("ğŸ”Œ [Apple STT] æ–·é–‹é€£æ¥")

        // åœæ­¢ä»»å‹™
        sourceTask?.cancel()
        targetTask?.cancel()
        sourceTask = nil
        targetTask = nil

        // çµæŸè«‹æ±‚
        sourceRequest?.endAudio()
        targetRequest?.endAudio()
        sourceRequest = nil
        targetRequest = nil

        // æ¸…é™¤è­˜åˆ¥å™¨
        sourceRecognizer = nil
        targetRecognizer = nil

        // åœæ­¢è¨ˆæ™‚å™¨
        debounceTimer?.invalidate()
        debounceTimer = nil
        taskRebuildTimer?.invalidate()
        taskRebuildTimer = nil

        // é‡ç½®ç‹€æ…‹
        lastSourceResult = nil
        lastTargetResult = nil
        lastEmittedText = ""
        lastEmittedLanguage = ""

        connectionState = .disconnected
    }

    func sendAudio(data: Data) {
        guard connectionState == .connected else { return }

        // è½‰æ› PCM Int16 â†’ AVAudioPCMBuffer
        guard let buffer = convertToAudioBuffer(data: data) else {
            return
        }

        // ç™¼é€çµ¦å…©å€‹è­˜åˆ¥å™¨
        sourceRequest?.append(buffer)
        targetRequest?.append(buffer)

        // çµ±è¨ˆéŸ³é »æ™‚é•·
        let duration = Double(buffer.frameLength) / 16000.0
        totalAudioDuration += duration
    }

    func sendEndUtterance() {
        // Apple STT æœƒè‡ªå‹•æª¢æ¸¬èªéŸ³çµæŸ
        // ä½†æˆ‘å€‘å¯ä»¥å¼·åˆ¶çµæŸç•¶å‰è­˜åˆ¥ä¸¦é‡å»ºä»»å‹™
        print("ğŸ“¤ [Apple STT] æ”¶åˆ°çµæŸèªå¥ä¿¡è™Ÿ")

        // ç«‹å³ç™¼é€ç•¶å‰æœ€ä½³çµæœï¼ˆå¦‚æœæœ‰ï¼‰
        emitBestResult(forceFinal: true)
    }

    // MARK: - Authorization

    private func handleAuthorizationStatus(_ status: SFSpeechRecognizerAuthorizationStatus) {
        switch status {
        case .authorized:
            print("âœ… [Apple STT] èªéŸ³è­˜åˆ¥å·²æˆæ¬Š")
            setupRecognizers()

        case .denied:
            connectionState = .error("èªéŸ³è­˜åˆ¥æ¬Šé™è¢«æ‹’çµ•")
            _errorSubject.send("è«‹åœ¨ã€Œè¨­å®š > éš±ç§æ¬Š > èªéŸ³è¾¨è­˜ã€ä¸­å…è¨±æ­¤ App")

        case .restricted:
            connectionState = .error("èªéŸ³è­˜åˆ¥å—é™")
            _errorSubject.send("æ­¤è¨­å‚™ä¸æ”¯æ´èªéŸ³è­˜åˆ¥")

        case .notDetermined:
            connectionState = .error("èªéŸ³è­˜åˆ¥æ¬Šé™æœªæ±ºå®š")
            _errorSubject.send("è«‹é‡æ–°å•Ÿå‹• App ä»¥è«‹æ±‚æ¬Šé™")

        @unknown default:
            connectionState = .error("æœªçŸ¥æ¬Šé™ç‹€æ…‹")
        }
    }

    // MARK: - Setup

    private func setupRecognizers() {
        print("ğŸ”§ [Apple STT] è¨­ç½®é›™èªè¨€è­˜åˆ¥å™¨")
        print("   ä¾†æºèªè¨€: \(sourceLang.displayName) (\(sourceLang.azureLocale))")
        print("   ç›®æ¨™èªè¨€: \(targetLang.displayName) (\(targetLang.azureLocale))")

        // å‰µå»ºè­˜åˆ¥å™¨
        let sourceLocale = Locale(identifier: sourceLang.azureLocale)
        let targetLocale = Locale(identifier: targetLang.azureLocale)

        sourceRecognizer = SFSpeechRecognizer(locale: sourceLocale)
        targetRecognizer = SFSpeechRecognizer(locale: targetLocale)

        // æª¢æŸ¥å¯ç”¨æ€§
        guard let sourceRec = sourceRecognizer else {
            connectionState = .error("ä¾†æºèªè¨€è­˜åˆ¥å™¨å‰µå»ºå¤±æ•—")
            _errorSubject.send("ä¸æ”¯æ´ \(sourceLang.displayName) èªéŸ³è­˜åˆ¥")
            return
        }

        guard let targetRec = targetRecognizer else {
            connectionState = .error("ç›®æ¨™èªè¨€è­˜åˆ¥å™¨å‰µå»ºå¤±æ•—")
            _errorSubject.send("ä¸æ”¯æ´ \(targetLang.displayName) èªéŸ³è­˜åˆ¥")
            return
        }

        guard sourceRec.isAvailable else {
            connectionState = .error("ä¾†æºèªè¨€è­˜åˆ¥å™¨ä¸å¯ç”¨")
            _errorSubject.send("è«‹ä¸‹è¼‰ \(sourceLang.displayName) èªè¨€åŒ…")
            return
        }

        guard targetRec.isAvailable else {
            connectionState = .error("ç›®æ¨™èªè¨€è­˜åˆ¥å™¨ä¸å¯ç”¨")
            _errorSubject.send("è«‹ä¸‹è¼‰ \(targetLang.displayName) èªè¨€åŒ…")
            return
        }

        // æª¢æŸ¥è¨­å‚™ç«¯è­˜åˆ¥æ”¯æ´
        let sourceOnDevice = sourceRec.supportsOnDeviceRecognition
        let targetOnDevice = targetRec.supportsOnDeviceRecognition

        print("   ä¾†æºèªè¨€è¨­å‚™ç«¯è­˜åˆ¥: \(sourceOnDevice ? "âœ… æ”¯æ´" : "âŒ ä¸æ”¯æ´")")
        print("   ç›®æ¨™èªè¨€è¨­å‚™ç«¯è­˜åˆ¥: \(targetOnDevice ? "âœ… æ”¯æ´" : "âŒ ä¸æ”¯æ´")")

        // å•Ÿå‹•è­˜åˆ¥ä»»å‹™
        startRecognitionTasks()

        // è¨­ç½®ä»»å‹™é‡å»ºè¨ˆæ™‚å™¨ï¼ˆé¿å… 1 åˆ†é˜é™åˆ¶ï¼‰
        setupTaskRebuildTimer()

        connectionState = .connected
        recognitionStartTime = Date()

        print("âœ… [Apple STT] é›™èªè¨€ä¸¦è¡Œè­˜åˆ¥å·²å•Ÿå‹•")
    }

    private func startRecognitionTasks() {
        guard let sourceRec = sourceRecognizer,
              let targetRec = targetRecognizer else {
            return
        }

        // å‰µå»ºè­˜åˆ¥è«‹æ±‚
        sourceRequest = SFSpeechAudioBufferRecognitionRequest()
        targetRequest = SFSpeechAudioBufferRecognitionRequest()

        guard let sourceReq = sourceRequest,
              let targetReq = targetRequest else {
            connectionState = .error("ç„¡æ³•å‰µå»ºè­˜åˆ¥è«‹æ±‚")
            return
        }

        // é…ç½®è«‹æ±‚
        sourceReq.shouldReportPartialResults = true
        targetReq.shouldReportPartialResults = true

        // â­ï¸ å¼·åˆ¶è¨­å‚™ç«¯è­˜åˆ¥ï¼ˆç„¡ API é…é¡é™åˆ¶ï¼‰
        if sourceRec.supportsOnDeviceRecognition {
            sourceReq.requiresOnDeviceRecognition = true
        }
        if targetRec.supportsOnDeviceRecognition {
            targetReq.requiresOnDeviceRecognition = true
        }

        // æ·»åŠ ä¸Šä¸‹æ–‡æç¤ºï¼ˆå¯é¸ï¼Œæé«˜æº–ç¢ºåº¦ï¼‰
        // sourceReq.contextualStrings = ["å¸¸ç”¨è©å½™"]

        // å•Ÿå‹•ä¾†æºèªè¨€è­˜åˆ¥ä»»å‹™
        sourceTask = sourceRec.recognitionTask(with: sourceReq) { [weak self] result, error in
            self?.handleRecognitionResult(
                result: result,
                error: error,
                isSource: true
            )
        }

        // å•Ÿå‹•ç›®æ¨™èªè¨€è­˜åˆ¥ä»»å‹™
        targetTask = targetRec.recognitionTask(with: targetReq) { [weak self] result, error in
            self?.handleRecognitionResult(
                result: result,
                error: error,
                isSource: false
            )
        }

        print("ğŸ™ï¸ [Apple STT] è­˜åˆ¥ä»»å‹™å·²å•Ÿå‹•")
    }

    private func setupTaskRebuildTimer() {
        taskRebuildTimer?.invalidate()
        taskRebuildTimer = Timer.scheduledTimer(
            withTimeInterval: taskRebuildInterval,
            repeats: true
        ) { [weak self] _ in
            self?.rebuildRecognitionTasks()
        }
    }

    /// é‡å»ºè­˜åˆ¥ä»»å‹™ï¼ˆé¿å… 1 åˆ†é˜è¶…æ™‚é™åˆ¶ï¼‰
    private func rebuildRecognitionTasks() {
        print("ğŸ”„ [Apple STT] é‡å»ºè­˜åˆ¥ä»»å‹™ï¼ˆé¿å…è¶…æ™‚ï¼‰")

        // ç™¼é€ç•¶å‰çµæœ
        emitBestResult(forceFinal: true)

        // çµæŸèˆŠä»»å‹™
        sourceTask?.cancel()
        targetTask?.cancel()
        sourceRequest?.endAudio()
        targetRequest?.endAudio()

        // é‡ç½®çµæœ
        lastSourceResult = nil
        lastTargetResult = nil

        // å•Ÿå‹•æ–°ä»»å‹™
        startRecognitionTasks()
    }

    // MARK: - Recognition Result Handling

    private func handleRecognitionResult(
        result: SFSpeechRecognitionResult?,
        error: Error?,
        isSource: Bool
    ) {
        let langName = isSource ? sourceLang.shortName : targetLang.shortName
        let langCode = isSource ? sourceLang.rawValue : targetLang.rawValue

        // è™•ç†éŒ¯èª¤
        if let error = error {
            // å¿½ç•¥å–æ¶ˆéŒ¯èª¤
            if (error as NSError).code == 1 || (error as NSError).code == 216 {
                // 1: kAFAssistantErrorDomain - ç”¨æˆ¶å–æ¶ˆ
                // 216: è­˜åˆ¥è¢«ä¸­æ–·
                return
            }
            print("âš ï¸ [Apple STT/\(langName)] éŒ¯èª¤: \(error.localizedDescription)")
            return
        }

        guard let result = result else { return }

        let text = result.bestTranscription.formattedString
        guard !text.isEmpty else { return }

        // è¨ˆç®—ä¿¡å¿ƒåº¦ï¼ˆä½¿ç”¨æœ€å¾Œä¸€å€‹ segment çš„ä¿¡å¿ƒåº¦ï¼‰
        let confidence = result.bestTranscription.segments.last?.confidence ?? 0
        let isFinal = result.isFinal

        // å‰µå»ºçµæœ
        let recognitionResult = RecognitionResult(
            text: text,
            confidence: confidence,
            language: langCode,
            isFinal: isFinal,
            timestamp: Date()
        )

        // æ›´æ–°å°æ‡‰èªè¨€çš„çµæœ
        if isSource {
            lastSourceResult = recognitionResult
        } else {
            lastTargetResult = recognitionResult
        }

        // Debug è¼¸å‡º
        let finalTag = isFinal ? "Final" : "Interim"
        print("ğŸ¤ [Apple STT/\(langName)] \(finalTag): \"\(text.prefix(40))\" (ä¿¡å¿ƒ: \(String(format: "%.2f", confidence)))")

        // é˜²æŠ–è™•ç†ï¼šåˆä½µçŸ­æ™‚é–“å…§çš„çµæœ
        scheduleResultEmission(isFinal: isFinal)
    }

    private func scheduleResultEmission(isFinal: Bool) {
        debounceTimer?.invalidate()

        if isFinal {
            // Final çµæœç«‹å³ç™¼é€
            emitBestResult(forceFinal: true)
        } else {
            // Interim çµæœé˜²æŠ–
            debounceTimer = Timer.scheduledTimer(
                withTimeInterval: debounceInterval,
                repeats: false
            ) { [weak self] _ in
                self?.emitBestResult(forceFinal: false)
            }
        }
    }

    /// æ ¹æ“šä¿¡å¿ƒåº¦é¸æ“‡æœ€ä½³çµæœä¸¦ç™¼é€
    private func emitBestResult(forceFinal: Bool) {
        // ç²å–å…©å€‹è­˜åˆ¥å™¨çš„çµæœ
        let sourceResult = lastSourceResult
        let targetResult = lastTargetResult

        // é¸æ“‡æœ€ä½³çµæœ
        guard let bestResult = selectBestResult(
            source: sourceResult,
            target: targetResult
        ) else {
            return
        }

        // å»é‡ï¼šé¿å…é‡è¤‡ç™¼é€ç›¸åŒå…§å®¹
        if bestResult.text == lastEmittedText && bestResult.language == lastEmittedLanguage && !forceFinal {
            return
        }

        let isFinal = forceFinal || bestResult.isFinal

        // æ›´æ–°å»é‡è¨˜éŒ„
        if isFinal {
            lastEmittedText = ""
            lastEmittedLanguage = ""
        } else {
            lastEmittedText = bestResult.text
            lastEmittedLanguage = bestResult.language
        }

        // å‰µå»º TranscriptMessage
        let transcript = TranscriptMessage(
            text: bestResult.text,
            isFinal: isFinal,
            confidence: Double(bestResult.confidence),
            language: bestResult.language
        )

        // ç™¼é€åˆ°ä¸»ç·šç¨‹
        DispatchQueue.main.async {
            self._transcriptSubject.send(transcript)
        }

        // Final çµæœè§¸ç™¼ç¿»è­¯
        if isFinal && !bestResult.text.isEmpty {
            translateText(text: bestResult.text, detectedLang: bestResult.language)

            // é‡ç½®çµæœ
            lastSourceResult = nil
            lastTargetResult = nil
        }
    }

    /// é¸æ“‡ä¿¡å¿ƒåº¦æœ€é«˜çš„çµæœ
    private func selectBestResult(
        source: RecognitionResult?,
        target: RecognitionResult?
    ) -> RecognitionResult? {

        // åªæœ‰ä¸€å€‹çµæœ
        guard let source = source else {
            return target
        }
        guard let target = target else {
            return source
        }

        // ä¿¡å¿ƒåº¦å·®ç•°é–¾å€¼ï¼ˆé¿å…å¾®å°å·®ç•°å°è‡´é »ç¹åˆ‡æ›ï¼‰
        let threshold: Float = 0.15

        // æ™‚é–“å·®ç•°é–¾å€¼ï¼ˆåªæ¯”è¼ƒè¿‘æœŸçµæœï¼‰
        let timeThreshold: TimeInterval = 0.5
        let timeDiff = abs(source.timestamp.timeIntervalSince(target.timestamp))

        // å¦‚æœæ™‚é–“å·®å¤ªå¤§ï¼Œé¸æ“‡è¼ƒæ–°çš„
        if timeDiff > timeThreshold {
            let winner = source.timestamp > target.timestamp ? source : target
            print("â±ï¸ [Apple STT] é¸æ“‡è¼ƒæ–°çµæœ: \(winner.language)")
            return winner
        }

        // æ¯”è¼ƒä¿¡å¿ƒåº¦
        if source.confidence > target.confidence + threshold {
            print("ğŸ† [Apple STT] é¸æ“‡ä¾†æºèªè¨€ (\(sourceLang.shortName)): ä¿¡å¿ƒ \(String(format: "%.2f", source.confidence)) > \(String(format: "%.2f", target.confidence))")
            return source
        } else if target.confidence > source.confidence + threshold {
            print("ğŸ† [Apple STT] é¸æ“‡ç›®æ¨™èªè¨€ (\(targetLang.shortName)): ä¿¡å¿ƒ \(String(format: "%.2f", target.confidence)) > \(String(format: "%.2f", source.confidence))")
            return target
        } else {
            // ä¿¡å¿ƒåº¦ç›¸è¿‘ï¼Œé¸æ“‡æ–‡æœ¬æ›´é•·çš„ï¼ˆé€šå¸¸æ›´å®Œæ•´ï¼‰
            if source.text.count >= target.text.count {
                print("ğŸ“ [Apple STT] ä¿¡å¿ƒåº¦ç›¸è¿‘ï¼Œé¸æ“‡è¼ƒé•·æ–‡æœ¬: \(sourceLang.shortName)")
                return source
            } else {
                print("ğŸ“ [Apple STT] ä¿¡å¿ƒåº¦ç›¸è¿‘ï¼Œé¸æ“‡è¼ƒé•·æ–‡æœ¬: \(targetLang.shortName)")
                return target
            }
        }
    }

    // MARK: - Translation

    private func translateText(text: String, detectedLang: String) {
        Task {
            await callTranslationAPI(text: text, detectedLang: detectedLang)
        }
    }

    private func callTranslationAPI(text: String, detectedLang: String) async {
        // ç¢ºå®šç¿»è­¯æ–¹å‘
        let isSourceLang = detectedLang == sourceLang.rawValue
        let translateTo = isSourceLang ? targetLang.rawValue : sourceLang.rawValue

        print("ğŸŒ [Apple STT] ç¿»è­¯: \(detectedLang) â†’ \(translateTo)")

        // æ§‹å»º API URL
        let urlString = "https://\(serverURL)/smart-translate"
        guard let url = URL(string: urlString) else {
            print("âŒ [Apple STT] ç„¡æ•ˆçš„ç¿»è­¯ URL")
            return
        }

        // æ§‹å»ºè«‹æ±‚
        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")

        let body: [String: Any] = [
            "text": text,
            "sourceLang": sourceLang.rawValue,
            "targetLang": targetLang.rawValue,
            "provider": translationProvider.rawValue
        ]

        do {
            request.httpBody = try JSONSerialization.data(withJSONObject: body)

            let (data, response) = try await URLSession.shared.data(for: request)

            guard let httpResponse = response as? HTTPURLResponse,
                  httpResponse.statusCode == 200 else {
                print("âŒ [Apple STT] ç¿»è­¯ API éŒ¯èª¤")
                return
            }

            // è§£æéŸ¿æ‡‰
            if let json = try JSONSerialization.jsonObject(with: data) as? [String: Any] {
                // å˜—è©¦è§£æ segments
                if let segmentsArray = json["segments"] as? [[String: Any]] {
                    var segments: [TranslationSegment] = []
                    for seg in segmentsArray {
                        if let original = seg["original"] as? String,
                           let translation = seg["translation"] as? String {
                            let isComplete = seg["isComplete"] as? Bool ?? true
                            segments.append(TranslationSegment(
                                original: original,
                                translation: translation,
                                isComplete: isComplete
                            ))
                        }
                    }

                    if !segments.isEmpty {
                        let fullTranslation = segments.map { $0.translation }.joined(separator: " ")

                        DispatchQueue.main.async {
                            self._segmentedTranslationSubject.send((text, segments))
                            self._translationSubject.send((text, fullTranslation))
                        }

                        print("âœ… [Apple STT] ç¿»è­¯å®Œæˆ: \"\(fullTranslation.prefix(40))...\"")
                        return
                    }
                }

                // å›é€€ï¼šä½¿ç”¨ç°¡å–®ç¿»è­¯
                if let translation = json["translation"] as? String {
                    DispatchQueue.main.async {
                        self._translationSubject.send((text, translation))
                    }
                    print("âœ… [Apple STT] ç¿»è­¯å®Œæˆ: \"\(translation.prefix(40))...\"")
                }
            }

        } catch {
            print("âŒ [Apple STT] ç¿»è­¯è«‹æ±‚å¤±æ•—: \(error.localizedDescription)")
        }
    }

    // MARK: - Audio Conversion

    /// å°‡ PCM Int16 Data è½‰æ›ç‚º AVAudioPCMBuffer
    private func convertToAudioBuffer(data: Data) -> AVAudioPCMBuffer? {
        // è¨ˆç®—å¹€æ•¸ï¼ˆ16-bit = 2 bytes per sampleï¼‰
        let frameCount = UInt32(data.count) / 2
        guard frameCount > 0 else { return nil }

        // å‰µå»º buffer
        guard let buffer = AVAudioPCMBuffer(
            pcmFormat: audioFormat,
            frameCapacity: frameCount
        ) else {
            return nil
        }
        buffer.frameLength = frameCount

        // è½‰æ› Int16 â†’ Float32
        data.withUnsafeBytes { (rawPtr: UnsafeRawBufferPointer) in
            guard let int16Ptr = rawPtr.baseAddress?.assumingMemoryBound(to: Int16.self),
                  let floatChannelData = buffer.floatChannelData else {
                return
            }

            let floatPtr = floatChannelData[0]
            for i in 0..<Int(frameCount) {
                floatPtr[i] = Float(int16Ptr[i]) / Float(Int16.max)
            }
        }

        return buffer
    }

    // MARK: - Static Methods

    /// æª¢æŸ¥èªè¨€æ˜¯å¦æ”¯æ´ Apple STT
    static func isLanguageSupported(_ language: Language) -> Bool {
        let locale = Locale(identifier: language.azureLocale)
        guard let recognizer = SFSpeechRecognizer(locale: locale) else {
            return false
        }
        return recognizer.isAvailable
    }

    /// æª¢æŸ¥èªè¨€æ˜¯å¦æ”¯æ´è¨­å‚™ç«¯è­˜åˆ¥
    static func supportsOnDeviceRecognition(_ language: Language) -> Bool {
        let locale = Locale(identifier: language.azureLocale)
        guard let recognizer = SFSpeechRecognizer(locale: locale) else {
            return false
        }
        return recognizer.supportsOnDeviceRecognition
    }

    /// åˆ—å‡ºæ‰€æœ‰æ”¯æ´çš„èªè¨€
    static func listSupportedLanguages() {
        print("ğŸ“‹ [Apple STT] æ”¯æ´çš„èªè¨€åˆ—è¡¨:")
        for lang in Language.allCases {
            if lang == .auto { continue }
            let supported = isLanguageSupported(lang)
            let onDevice = supportsOnDeviceRecognition(lang)
            let status = supported ? (onDevice ? "âœ… è¨­å‚™ç«¯" : "â˜ï¸ é›²ç«¯") : "âŒ ä¸æ”¯æ´"
            print("   \(lang.displayName): \(status)")
        }
    }
}
