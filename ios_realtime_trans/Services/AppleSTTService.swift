//
//  AppleSTTService.swift
//  ios_realtime_trans
//
//  Apple å…§å»º STT æœå‹™ï¼ˆé›™èªè¨€ä¸¦è¡Œè­˜åˆ¥ï¼‰
//  ä½¿ç”¨ SFSpeechRecognizer é€²è¡Œè¨­å‚™ç«¯èªéŸ³è­˜åˆ¥
//  å„ªé»ï¼šå…è²»ã€é›¢ç·šå¯ç”¨ã€ä½å»¶é²ã€ç„¡ API é…é¡é™åˆ¶
//  ç¼ºé»ï¼šèªè¨€æ”¯æ´è¼ƒå°‘ã€éœ€è¦ç”¨æˆ¶ä¸‹è¼‰èªè¨€åŒ…
//

import Foundation
import Speech
import AVFoundation
import Combine

/// Apple STT æœå‹™ï¼ˆè¨­å‚™ç«¯é›™èªè¨€ä¸¦è¡Œè­˜åˆ¥ï¼‰
class AppleSTTService: NSObject, WebSocketServiceProtocol {

    // MARK: - Properties

    /// é›™èªè¨€è­˜åˆ¥å™¨
    private var sourceRecognizer: SFSpeechRecognizer?
    private var targetRecognizer: SFSpeechRecognizer?

    /// è­˜åˆ¥ä»»å‹™
    private var sourceTask: SFSpeechRecognitionTask?
    private var targetTask: SFSpeechRecognitionTask?

    /// è­˜åˆ¥è«‹æ±‚
    private var sourceRequest: SFSpeechAudioBufferRecognitionRequest?
    private var targetRequest: SFSpeechAudioBufferRecognitionRequest?

    /// ç•¶å‰èªè¨€è¨­ç½®
    private var sourceLang: Language = .zh
    private var targetLang: Language = .en

    /// â­ï¸ ç¶“æ¿Ÿæ¨¡å¼ï¼šå–®èªè¨€æ¨¡å¼
    private(set) var isSingleLanguageMode: Bool = false
    private(set) var currentActiveLanguage: Language = .zh

    // MARK: - â­ï¸ è‡ªå‹•èªè¨€åˆ‡æ›ï¼ˆç¶“æ¿Ÿæ¨¡å¼ï¼‰

    /// éŸ³é »ç’°å½¢ç·©è¡å€ï¼ˆå„²å­˜æœ€è¿‘ 5 ç§’çš„éŸ³é »ï¼‰
    private let audioRingBuffer = AudioRingBuffer(capacitySeconds: 5.0, sampleRate: 16000)

    /// æ˜¯å¦å•Ÿç”¨è‡ªå‹•èªè¨€åˆ‡æ›
    var isAutoLanguageSwitchEnabled: Bool = true

    /// ä¿¡å¿ƒåº¦é–¾å€¼ï¼ˆä½æ–¼æ­¤å€¼è§¸ç™¼åˆ‡æ›ï¼‰
    var confidenceThreshold: Float = 0.70

    /// æ˜¯å¦æ­£åœ¨é€²è¡Œèªè¨€æ¯”è¼ƒ
    private var isComparingLanguages: Bool = false

    /// æ¯”è¼ƒä¸­çš„çµæœæš«å­˜
    private var comparisonResults: [Language: (text: String, confidence: Float)] = [:]

    /// æ¯”è¼ƒå®Œæˆå¾Œçš„å›èª¿ï¼ˆç”¨æ–¼ UI æ›´æ–°ï¼‰
    var onLanguageComparisonComplete: ((Language, String, Float) -> Void)?

    /// èªè¨€åˆ‡æ›å›èª¿ï¼ˆç”¨æ–¼åŒæ­¥ UIï¼‰
    var onLanguageSwitched: ((Language) -> Void)?

    /// ä½ä¿¡å¿ƒåº¦é–¾å€¼ï¼ˆä½æ–¼æ­¤å€¼çš„ Final è¦–ç‚ºä¸å¯é ï¼‰
    private let unreliableFinalThreshold: Float = 0.30

    /// â­ï¸ æ¯”è¼ƒé¡¯ç¤ºæ¨¡å¼ï¼šå¼·åˆ¶å…©ç¨®èªè¨€éƒ½è¾¨è­˜ä¸€æ¬¡ï¼Œä¸¦é¡¯ç¤ºå…©å€‹çµæœ
    /// ç”¨æ–¼èª¿è©¦å’Œæ¯”è¼ƒå…©ç¨®èªè¨€çš„è¾¨è­˜æ•ˆæœ
    var isComparisonDisplayMode: Bool = false

    /// æ¯”è¼ƒçµæœå›èª¿ï¼ˆé¡¯ç¤ºå…©å€‹èªè¨€çš„çµæœï¼‰- èˆŠç‰ˆï¼ˆæ¯”è¼ƒé¡¯ç¤ºæ¨¡å¼ç”¨ï¼‰
    var onComparisonResults: ((_ results: [(lang: Language, text: String, confidence: Float, isFinal: Bool)]) -> Void)?

    /// â­ï¸ æœ€ä½³æ¯”è¼ƒçµæœå›èª¿ï¼ˆç¶“æ¿Ÿæ¨¡å¼ PTT ç”¨ï¼‰
    /// é¸æ“‡ä¿¡å¿ƒæ°´æº–æœ€é«˜çš„èªè¨€ï¼Œä¸¦è§¸ç™¼ç¿»è­¯ + TTS
    var onBestComparisonResult: ((_ bestLang: Language, _ text: String, _ confidence: Float) -> Void)?

    /// é€£æ¥ç‹€æ…‹
    private(set) var connectionState: WebSocketConnectionState = .disconnected

    /// ç¿»è­¯æ¨¡å‹é¸æ“‡
    var translationProvider: TranslationProvider = .grok

    /// ä¼ºæœå™¨ URLï¼ˆç”¨æ–¼ç¿»è­¯ APIï¼‰
    private var serverURL: String = ""

    // MARK: - Publishers

    private let _transcriptSubject = PassthroughSubject<TranscriptMessage, Never>()
    private let _translationSubject = PassthroughSubject<(String, String), Never>()
    private let _segmentedTranslationSubject = PassthroughSubject<(String, [TranslationSegment]), Never>()
    private let _correctionSubject = PassthroughSubject<(String, String), Never>()
    private let _errorSubject = PassthroughSubject<String, Never>()

    var transcriptPublisher: AnyPublisher<TranscriptMessage, Never> {
        _transcriptSubject.eraseToAnyPublisher()
    }
    var translationPublisher: AnyPublisher<(String, String), Never> {
        _translationSubject.eraseToAnyPublisher()
    }
    var segmentedTranslationPublisher: AnyPublisher<(String, [TranslationSegment]), Never> {
        _segmentedTranslationSubject.eraseToAnyPublisher()
    }
    var correctionPublisher: AnyPublisher<(String, String), Never> {
        _correctionSubject.eraseToAnyPublisher()
    }
    var errorPublisher: AnyPublisher<String, Never> {
        _errorSubject.eraseToAnyPublisher()
    }

    // MARK: - ä¿¡å¿ƒåº¦è¿½è¹¤

    /// æœ€æ–°çš„ä¾†æºèªè¨€è­˜åˆ¥çµæœ
    private var lastSourceResult: RecognitionResult?
    /// æœ€æ–°çš„ç›®æ¨™èªè¨€è­˜åˆ¥çµæœ
    private var lastTargetResult: RecognitionResult?

    /// è­˜åˆ¥çµæœçµæ§‹
    private struct RecognitionResult {
        let text: String
        let confidence: Float
        let language: String
        let isFinal: Bool
        let timestamp: Date
    }

    /// ä¸Šä¸€æ¬¡ç™¼é€çš„çµæœï¼ˆç”¨æ–¼å»é‡ï¼‰
    private var lastEmittedText: String = ""
    private var lastEmittedLanguage: String = ""

    // MARK: - é˜²æŠ–èˆ‡è¶…æ™‚

    /// é˜²æŠ–è¨ˆæ™‚å™¨ï¼ˆåˆä½µçŸ­æ™‚é–“å…§çš„çµæœï¼‰
    private var debounceTimer: Timer?
    private let debounceInterval: TimeInterval = 0.15  // 150ms é˜²æŠ–

    /// ä»»å‹™é‡å»ºè¨ˆæ™‚å™¨ï¼ˆApple STT æœ‰ç´„ 1 åˆ†é˜é™åˆ¶ï¼‰
    private var taskRebuildTimer: Timer?
    private let taskRebuildInterval: TimeInterval = 55.0  // 55 ç§’é‡å»º

    /// éŸ³é »æ ¼å¼
    private let audioFormat = AVAudioFormat(
        commonFormat: .pcmFormatFloat32,
        sampleRate: 16000,
        channels: 1,
        interleaved: false
    )!

    // MARK: - çµ±è¨ˆ

    private var recognitionStartTime: Date?
    private var totalAudioDuration: TimeInterval = 0

    /// â­ï¸ è¾¨è­˜å»¶é²çµ±è¨ˆ
    private var lastAudioSendTime: Date?           // æœ€å¾Œä¸€æ¬¡ç™¼é€éŸ³é »çš„æ™‚é–“
    private var audioSendTimestamps: [Date] = []   // éŸ³é »ç™¼é€æ™‚é–“è¨˜éŒ„ï¼ˆæœ€è¿‘ 10 å€‹ï¼‰
    private var recognitionLatencies: [TimeInterval] = []  // è¾¨è­˜å»¶é²è¨˜éŒ„ï¼ˆmsï¼‰

    // MARK: - é˜²æ­¢ç„¡é™é‡å•Ÿ

    /// é‡å»ºå†·å»æ™‚é–“ï¼ˆé˜²æ­¢å¿«é€Ÿå¾ªç’°é‡å•Ÿï¼‰
    private var lastRebuildTime: Date?
    private let rebuildCooldown: TimeInterval = 3.0  // è‡³å°‘ 3 ç§’æ‰èƒ½é‡å»º

    /// é€£çºŒéŒ¯èª¤è¨ˆæ•¸ï¼ˆç”¨æ–¼æ±ºå®šæ˜¯å¦æ”¾æ£„ï¼‰
    private var consecutiveErrorCount = 0
    private let maxConsecutiveErrors = 5

    // MARK: - Initialization

    override init() {
        super.init()
        print("âœ… [Apple STT] æœå‹™åˆå§‹åŒ–")
    }

    // MARK: - WebSocketServiceProtocol

    func connect(serverURL: String, sourceLang: Language, targetLang: Language) {
        self.serverURL = serverURL
        self.sourceLang = sourceLang
        self.targetLang = targetLang
        self.isSingleLanguageMode = false  // é è¨­ç‚ºé›™èªè¨€æ¨¡å¼

        connectionState = .connecting

        // è«‹æ±‚èªéŸ³è­˜åˆ¥æ¬Šé™
        SFSpeechRecognizer.requestAuthorization { [weak self] status in
            DispatchQueue.main.async {
                self?.handleAuthorizationStatus(status)
            }
        }
    }

    // MARK: - â­ï¸ ç¶“æ¿Ÿæ¨¡å¼ï¼šå–®èªè¨€æ¨¡å¼

    /// å–®èªè¨€æ¨¡å¼é€£æ¥ï¼ˆç¶“æ¿Ÿæ¨¡å¼ç”¨ï¼‰
    /// åªé–‹å•Ÿä¸€å€‹èªè¨€è­˜åˆ¥å™¨ï¼Œç¯€çœè³‡æº
    func connectSingleLanguage(
        serverURL: String,
        sourceLang: Language,
        targetLang: Language,
        activeLanguage: Language
    ) {
        self.serverURL = serverURL
        self.sourceLang = sourceLang
        self.targetLang = targetLang
        self.isSingleLanguageMode = true
        self.currentActiveLanguage = activeLanguage

        connectionState = .connecting

        print("ğŸŒ¿ [Apple STT] ç¶“æ¿Ÿæ¨¡å¼ï¼šå–®èªè¨€é€£æ¥ (\(activeLanguage.shortName))")

        // è«‹æ±‚èªéŸ³è­˜åˆ¥æ¬Šé™
        SFSpeechRecognizer.requestAuthorization { [weak self] status in
            DispatchQueue.main.async {
                self?.handleAuthorizationStatus(status)
            }
        }
    }

    /// åˆ‡æ›èªè¨€ï¼ˆç¶“æ¿Ÿæ¨¡å¼å°ˆç”¨ï¼‰
    /// è¿”å›åˆ‡æ›è€—æ™‚ï¼ˆæ¯«ç§’ï¼‰
    @discardableResult
    func switchLanguage(to language: Language) -> TimeInterval {
        guard isSingleLanguageMode else {
            print("âš ï¸ [Apple STT] switchLanguage åªèƒ½åœ¨å–®èªè¨€æ¨¡å¼ä¸‹ä½¿ç”¨")
            return 0
        }

        guard language != currentActiveLanguage else {
            print("â„¹ï¸ [Apple STT] å·²ç¶“æ˜¯ \(language.shortName)ï¼Œç„¡éœ€åˆ‡æ›")
            return 0
        }

        let startTime = Date()
        print("ğŸ”„ [Apple STT] é–‹å§‹åˆ‡æ›èªè¨€: \(currentActiveLanguage.shortName) â†’ \(language.shortName)")

        // åœæ­¢ç•¶å‰è­˜åˆ¥
        stopSingleLanguageRecognition()

        // æ›´æ–°ç•¶å‰èªè¨€
        currentActiveLanguage = language

        // å•Ÿå‹•æ–°èªè¨€è­˜åˆ¥
        startSingleLanguageRecognition()

        // è¨ˆç®—åˆ‡æ›æ™‚é–“
        let switchTime = Date().timeIntervalSince(startTime) * 1000  // è½‰æ›ç‚ºæ¯«ç§’
        print("â±ï¸ [Apple STT] èªè¨€åˆ‡æ›å®Œæˆï¼Œè€—æ™‚: \(String(format: "%.0f", switchTime))ms")

        return switchTime
    }

    /// åœæ­¢å–®èªè¨€è­˜åˆ¥
    private func stopSingleLanguageRecognition() {
        sourceTask?.cancel()
        sourceTask = nil
        sourceRequest?.endAudio()
        sourceRequest = nil
        sourceRecognizer = nil

        // é‡ç½®çµæœ
        lastSourceResult = nil
        lastEmittedText = ""
        lastEmittedLanguage = ""
    }

    /// å•Ÿå‹•å–®èªè¨€è­˜åˆ¥
    private func startSingleLanguageRecognition() {
        let locale = Locale(identifier: currentActiveLanguage.azureLocale)
        sourceRecognizer = SFSpeechRecognizer(locale: locale)

        guard let recognizer = sourceRecognizer else {
            connectionState = .error("\(currentActiveLanguage.displayName) è­˜åˆ¥å™¨å‰µå»ºå¤±æ•—")
            _errorSubject.send("ä¸æ”¯æ´ \(currentActiveLanguage.displayName) èªéŸ³è­˜åˆ¥")
            return
        }

        guard recognizer.isAvailable else {
            connectionState = .error("\(currentActiveLanguage.displayName) è­˜åˆ¥å™¨ä¸å¯ç”¨")
            _errorSubject.send("è«‹ä¸‹è¼‰ \(currentActiveLanguage.displayName) èªè¨€åŒ…")
            return
        }

        // å‰µå»ºè­˜åˆ¥è«‹æ±‚
        sourceRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let request = sourceRequest else {
            connectionState = .error("ç„¡æ³•å‰µå»ºè­˜åˆ¥è«‹æ±‚")
            return
        }

        request.shouldReportPartialResults = true
        if recognizer.supportsOnDeviceRecognition {
            request.requiresOnDeviceRecognition = true
        }

        // å•Ÿå‹•è­˜åˆ¥ä»»å‹™
        sourceTask = recognizer.recognitionTask(with: request) { [weak self] result, error in
            self?.handleSingleLanguageResult(result: result, error: error)
        }

        connectionState = .connected
        print("âœ… [Apple STT] å–®èªè¨€è­˜åˆ¥å·²å•Ÿå‹•: \(currentActiveLanguage.shortName)")
    }

    /// è™•ç†å–®èªè¨€è­˜åˆ¥çµæœ
    private func handleSingleLanguageResult(
        result: SFSpeechRecognitionResult?,
        error: Error?
    ) {
        // è¤‡ç”¨ç¾æœ‰çš„éŒ¯èª¤è™•ç†é‚è¼¯
        if let error = error {
            let nsError = error as NSError
            if nsError.code == 1 || nsError.code == 216 { return }

            if nsError.code == 1110 {
                consecutiveErrorCount += 1
                if consecutiveErrorCount == 1 {
                    print("â„¹ï¸ [Apple STT/\(currentActiveLanguage.shortName)] ç­‰å¾…èªéŸ³è¼¸å…¥...")
                }
                return
            }

            print("âš ï¸ [Apple STT/\(currentActiveLanguage.shortName)] éŒ¯èª¤: \(error.localizedDescription)")
            return
        }

        consecutiveErrorCount = 0

        guard let result = result else { return }

        let text = result.bestTranscription.formattedString
        guard !text.isEmpty else { return }

        let confidence = result.bestTranscription.segments.last?.confidence ?? 0
        let isFinal = result.isFinal

        // å»é‡
        if text == lastEmittedText && !isFinal { return }

        // â­ï¸ è¨ˆç®—è¾¨è­˜å»¶é²
        var latencyInfo = ""
        if let lastSend = lastAudioSendTime {
            let latency = Date().timeIntervalSince(lastSend) * 1000  // ms
            recognitionLatencies.append(latency)
            if recognitionLatencies.count > 20 {
                recognitionLatencies.removeFirst()
            }
            let avgLatency = recognitionLatencies.reduce(0, +) / Double(recognitionLatencies.count)
            latencyInfo = " | å»¶é²: \(String(format: "%.0f", latency))ms (å¹³å‡: \(String(format: "%.0f", avgLatency))ms)"
        }

        // â­ï¸ Interim çµæœæ²’æœ‰ä¿¡å¿ƒåº¦ï¼ˆApple çš„è¨­è¨ˆï¼‰ï¼Œåªæœ‰ Final æ‰é¡¯ç¤º
        let confidenceInfo = isFinal ? " (ä¿¡å¿ƒ: \(String(format: "%.2f", confidence)))" : ""
        let finalTag = isFinal ? "âœ… Final" : "â³ Interim"
        print("ğŸ¤ [Apple STT/\(currentActiveLanguage.shortName)] \(finalTag): \"\(text.prefix(40))\"\(confidenceInfo)\(latencyInfo)")

        // å‰µå»º TranscriptMessage
        let transcript = TranscriptMessage(
            text: text,
            isFinal: isFinal,
            confidence: Double(confidence),
            language: currentActiveLanguage.rawValue
        )

        if isFinal {
            lastEmittedText = ""

            // â­ï¸ æ¯”è¼ƒé¡¯ç¤ºæ¨¡å¼ï¼šå¼·åˆ¶å…©ç¨®èªè¨€éƒ½è¾¨è­˜ä¸€æ¬¡
            if isSingleLanguageMode && isComparisonDisplayMode && !isComparingLanguages {
                print("ğŸ”¬ [æ¯”è¼ƒæ¨¡å¼] æ”¶åˆ° Finalï¼Œé–‹å§‹å¼·åˆ¶æ¯”è¼ƒå…©ç¨®èªè¨€")
                comparisonResults[currentActiveLanguage] = (text: text, confidence: confidence)
                startLanguageComparison()
                return
            }

            // â­ï¸ è‡ªå‹•èªè¨€åˆ‡æ›é‚è¼¯
            if isSingleLanguageMode && isAutoLanguageSwitchEnabled && !isComparisonDisplayMode && !isComparingLanguages {
                // æª¢æŸ¥ä¿¡å¿ƒåº¦æ˜¯å¦ä½æ–¼é–¾å€¼
                // æ³¨æ„ï¼šApple STT æœ‰æ™‚ Final ä¿¡å¿ƒåº¦ç‚º 0ï¼ˆbugï¼‰ï¼Œé€™ç¨®æƒ…æ³ä¹Ÿè¦è§¸ç™¼åˆ‡æ›
                let shouldSwitch = confidence < confidenceThreshold || confidence == 0
                if shouldSwitch {
                    print("âš ï¸ [è‡ªå‹•åˆ‡æ›] ä¿¡å¿ƒåº¦ \(String(format: "%.2f", confidence)) < \(String(format: "%.2f", confidenceThreshold))ï¼Œå˜—è©¦å¦ä¸€ç¨®èªè¨€")

                    // å„²å­˜ç•¶å‰çµæœ
                    comparisonResults[currentActiveLanguage] = (text: text, confidence: confidence)

                    // è§¸ç™¼èªè¨€æ¯”è¼ƒ
                    startLanguageComparison()
                    return  // ä¸ç™¼é€çµæœï¼Œç­‰å¾…æ¯”è¼ƒå®Œæˆ
                }
            }

            // å¦‚æœæ˜¯æ¯”è¼ƒæ¨¡å¼ï¼Œè™•ç†æ¯”è¼ƒçµæœ
            if isComparingLanguages {
                handleComparisonResult(text: text, confidence: confidence, isFinal: true)
                return
            }

            // æ­£å¸¸æ¨¡å¼ï¼šè§¸ç™¼ç¿»è­¯
            translateText(text: text, detectedLang: currentActiveLanguage.rawValue)
        } else {
            lastEmittedText = text

            // â­ï¸ æ¯”è¼ƒæ¨¡å¼ä¸‹ä¹Ÿä¿å­˜ Interim çµæœï¼ˆä½œç‚ºå‚™ç”¨ï¼‰
            if isComparingLanguages {
                // ä½¿ç”¨è² æ•¸ä¿¡å¿ƒåº¦æ¨™è¨˜ç‚º Interimï¼ˆInterim æ²’æœ‰ä¿¡å¿ƒåº¦ï¼‰
                comparisonResults[currentActiveLanguage] = (text: text, confidence: -1.0)
                print("ğŸ“ [è‡ªå‹•åˆ‡æ›] æš«å­˜ Interim: \(currentActiveLanguage.shortName) = \"\(text.prefix(20))...\"")
            }
        }

        DispatchQueue.main.async {
            self._transcriptSubject.send(transcript)
        }
    }

    func disconnect() {
        print("ğŸ”Œ [Apple STT] æ–·é–‹é€£æ¥")

        // åœæ­¢ä»»å‹™
        sourceTask?.cancel()
        targetTask?.cancel()
        sourceTask = nil
        targetTask = nil

        // çµæŸè«‹æ±‚
        sourceRequest?.endAudio()
        targetRequest?.endAudio()
        sourceRequest = nil
        targetRequest = nil

        // æ¸…é™¤è­˜åˆ¥å™¨
        sourceRecognizer = nil
        targetRecognizer = nil

        // åœæ­¢è¨ˆæ™‚å™¨
        debounceTimer?.invalidate()
        debounceTimer = nil
        taskRebuildTimer?.invalidate()
        taskRebuildTimer = nil

        // é‡ç½®ç‹€æ…‹
        lastSourceResult = nil
        lastTargetResult = nil
        lastEmittedText = ""
        lastEmittedLanguage = ""

        // é‡ç½®è¨ˆæ•¸å™¨
        audioSendCount = 0
        sourceErrorCount = 0
        targetErrorCount = 0

        // â­ï¸ é‡ç½®è‡ªå‹•åˆ‡æ›ç‹€æ…‹
        isComparingLanguages = false
        comparisonResults.removeAll()
        audioRingBuffer.clear()
        consecutiveErrorCount = 0
        lastRebuildTime = nil

        // â­ï¸ é‡ç½®å»¶é²çµ±è¨ˆ
        lastAudioSendTime = nil
        audioSendTimestamps.removeAll()
        recognitionLatencies.removeAll()
        totalAudioDuration = 0

        connectionState = .disconnected
    }

    /// éŸ³é »ç™¼é€è¨ˆæ•¸å™¨
    private var audioSendCount = 0

    func sendAudio(data: Data) {
        // â­ï¸ ç¬¬ä¸€æ¬¡èª¿ç”¨æ™‚æ‰“å°è©³ç´°ç‹€æ…‹
        if audioSendCount == 0 {
            print("ğŸ” [Apple STT] sendAudio é¦–æ¬¡èª¿ç”¨:")
            print("   connectionState: \(connectionState)")
            print("   sourceRequest: \(sourceRequest != nil ? "å­˜åœ¨" : "nil")")
            print("   targetRequest: \(targetRequest != nil ? "å­˜åœ¨" : "nil")")
            print("   data.count: \(data.count) bytes")
        }

        guard connectionState == .connected else {
            if audioSendCount % 50 == 0 {  // æ¸›å°‘ log å™ªéŸ³
                print("âš ï¸ [Apple STT] sendAudio: æœªé€£æ¥ (\(connectionState))ï¼Œå¿½ç•¥")
            }
            audioSendCount += 1
            return
        }

        audioSendCount += 1

        // â­ï¸ ç¶“æ¿Ÿæ¨¡å¼ï¼šå„²å­˜éŸ³é »åˆ°ç’°å½¢ç·©è¡å€ï¼ˆç”¨æ–¼è‡ªå‹•èªè¨€åˆ‡æ›é‡è©¦ï¼‰
        if isSingleLanguageMode && isAutoLanguageSwitchEnabled && !isComparingLanguages {
            audioRingBuffer.write(data)
        }

        // æ¯ 20 æ¬¡æ‰“å°ä¸€æ¬¡ debug info
        if audioSendCount == 1 || audioSendCount % 20 == 0 {
            print("ğŸ“¤ [Apple STT] æ”¶åˆ°éŸ³é » #\(audioSendCount): \(data.count) bytes")
            if isSingleLanguageMode && isAutoLanguageSwitchEnabled {
                print("   ğŸ“¼ ç·©è¡å€: \(String(format: "%.1f", audioRingBuffer.bufferedDuration))ç§’")
            }
        }

        // è½‰æ› PCM Int16 â†’ AVAudioPCMBuffer
        guard let buffer = convertToAudioBuffer(data: data) else {
            print("âŒ [Apple STT] éŸ³é »è½‰æ›å¤±æ•— (data.count: \(data.count))")
            return
        }

        // â­ï¸ æ ¹æ“šæ¨¡å¼æª¢æŸ¥ request
        if isSingleLanguageMode {
            // å–®èªè¨€æ¨¡å¼ï¼šåªéœ€è¦ sourceRequest
            guard let srcReq = sourceRequest else {
                print("âŒ [Apple STT] å–®èªè¨€æ¨¡å¼ï¼šRequest ç‚ºç©º")
                return
            }
            srcReq.append(buffer)
        } else {
            // é›™èªè¨€æ¨¡å¼ï¼šéœ€è¦å…©å€‹ request
            guard let srcReq = sourceRequest, let tgtReq = targetRequest else {
                print("âŒ [Apple STT] Request ç‚ºç©ºï¼Œç„¡æ³•ç™¼é€éŸ³é »")
                print("   sourceRequest: \(sourceRequest != nil)")
                print("   targetRequest: \(targetRequest != nil)")
                return
            }
            srcReq.append(buffer)
            tgtReq.append(buffer)
        }

        // â­ï¸ è¨ˆç®—éŸ³é »æŒ¯å¹…ï¼ˆèª¿è©¦ç”¨ï¼‰
        var maxAmplitude: Float = 0
        var avgAmplitude: Float = 0
        if let floatData = buffer.floatChannelData?[0] {
            let frameCount = Int(buffer.frameLength)
            var sum: Float = 0
            for i in 0..<frameCount {
                let absVal = abs(floatData[i])
                sum += absVal
                if absVal > maxAmplitude {
                    maxAmplitude = absVal
                }
            }
            avgAmplitude = sum / Float(frameCount)
        }

        // çµ±è¨ˆéŸ³é »æ™‚é•·
        let duration = Double(buffer.frameLength) / 16000.0
        totalAudioDuration += duration

        // â­ï¸ è¨˜éŒ„ç™¼é€æ™‚é–“ï¼ˆç”¨æ–¼è¨ˆç®—è¾¨è­˜å»¶é²ï¼‰
        let now = Date()
        lastAudioSendTime = now
        audioSendTimestamps.append(now)
        if audioSendTimestamps.count > 10 {
            audioSendTimestamps.removeFirst()
        }

        // æ¯ 20 æ¬¡æ‰“å°ä¸€æ¬¡ï¼ˆåŒ…å«æŒ¯å¹…è³‡è¨Šï¼‰
        if audioSendCount == 1 || audioSendCount % 20 == 0 {
            print("   âœ… å·²ç™¼é€åˆ°è­˜åˆ¥å™¨ (ç´¯è¨ˆ \(String(format: "%.1f", totalAudioDuration))ç§’)")
            print("   ğŸ“Š æŒ¯å¹…: max=\(String(format: "%.4f", maxAmplitude)), avg=\(String(format: "%.6f", avgAmplitude))")

            // æŒ¯å¹…è­¦å‘Š
            if maxAmplitude < 0.01 {
                print("   âš ï¸ éŸ³é »æŒ¯å¹…éä½ï¼å¯èƒ½æ˜¯éœéŸ³æˆ–éº¥å…‹é¢¨å•é¡Œ")
            }
        }
    }

    func sendEndUtterance() {
        // Apple STT æœƒè‡ªå‹•æª¢æ¸¬èªéŸ³çµæŸ
        // ä½†æˆ‘å€‘å¯ä»¥å¼·åˆ¶çµæŸç•¶å‰è­˜åˆ¥ä¸¦é‡å»ºä»»å‹™
        print("ğŸ“¤ [Apple STT] æ”¶åˆ°çµæŸèªå¥ä¿¡è™Ÿ")

        // â­ï¸ å¦‚æœæ­£åœ¨æ¯”è¼ƒæ¨¡å¼ï¼Œä¸è¦å¹²æ“¾æ¯”è¼ƒæµç¨‹
        if isComparingLanguages {
            print("â¸ï¸ [Apple STT] æ¯”è¼ƒæ¨¡å¼ä¸­ï¼Œå¿½ç•¥çµæŸä¿¡è™Ÿ")
            return
        }

        if isSingleLanguageMode {
            // â­ï¸ å–®èªè¨€æ¨¡å¼ï¼šå¼·åˆ¶çµæŸä¸¦é‡å»ºè­˜åˆ¥
            print("ğŸ”š [Apple STT] å–®èªè¨€æ¨¡å¼ï¼šå¼·åˆ¶çµæŸè­˜åˆ¥")
            sourceRequest?.endAudio()

            // çŸ­æš«å»¶é²å¾Œé‡å»ºï¼ˆè®“ Final çµæœè¿”å›ï¼‰
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.3) { [weak self] in
                guard let self = self, self.connectionState == .connected else { return }
                // å†æ¬¡æª¢æŸ¥æ˜¯å¦åœ¨æ¯”è¼ƒæ¨¡å¼
                guard !self.isComparingLanguages else { return }
                self.startSingleLanguageRecognition()
            }
        } else {
            // é›™èªè¨€æ¨¡å¼ï¼šç«‹å³ç™¼é€ç•¶å‰æœ€ä½³çµæœ
            emitBestResult(forceFinal: true)
        }
    }

    // MARK: - Authorization

    private func handleAuthorizationStatus(_ status: SFSpeechRecognizerAuthorizationStatus) {
        switch status {
        case .authorized:
            print("âœ… [Apple STT] èªéŸ³è­˜åˆ¥å·²æˆæ¬Š")
            // â­ï¸ æ ¹æ“šæ¨¡å¼å•Ÿå‹•ä¸åŒçš„è­˜åˆ¥å™¨
            if isSingleLanguageMode {
                startSingleLanguageRecognition()
            } else {
                setupRecognizers()
            }

        case .denied:
            connectionState = .error("èªéŸ³è­˜åˆ¥æ¬Šé™è¢«æ‹’çµ•")
            _errorSubject.send("è«‹åœ¨ã€Œè¨­å®š > éš±ç§æ¬Š > èªéŸ³è¾¨è­˜ã€ä¸­å…è¨±æ­¤ App")

        case .restricted:
            connectionState = .error("èªéŸ³è­˜åˆ¥å—é™")
            _errorSubject.send("æ­¤è¨­å‚™ä¸æ”¯æ´èªéŸ³è­˜åˆ¥")

        case .notDetermined:
            connectionState = .error("èªéŸ³è­˜åˆ¥æ¬Šé™æœªæ±ºå®š")
            _errorSubject.send("è«‹é‡æ–°å•Ÿå‹• App ä»¥è«‹æ±‚æ¬Šé™")

        @unknown default:
            connectionState = .error("æœªçŸ¥æ¬Šé™ç‹€æ…‹")
        }
    }

    // MARK: - Setup

    private func setupRecognizers() {
        print("ğŸ”§ [Apple STT] è¨­ç½®é›™èªè¨€è­˜åˆ¥å™¨")
        print("   ä¾†æºèªè¨€: \(sourceLang.displayName) (\(sourceLang.azureLocale))")
        print("   ç›®æ¨™èªè¨€: \(targetLang.displayName) (\(targetLang.azureLocale))")

        // å‰µå»ºè­˜åˆ¥å™¨
        let sourceLocale = Locale(identifier: sourceLang.azureLocale)
        let targetLocale = Locale(identifier: targetLang.azureLocale)

        sourceRecognizer = SFSpeechRecognizer(locale: sourceLocale)
        targetRecognizer = SFSpeechRecognizer(locale: targetLocale)

        // æª¢æŸ¥å¯ç”¨æ€§
        guard let sourceRec = sourceRecognizer else {
            connectionState = .error("ä¾†æºèªè¨€è­˜åˆ¥å™¨å‰µå»ºå¤±æ•—")
            _errorSubject.send("ä¸æ”¯æ´ \(sourceLang.displayName) èªéŸ³è­˜åˆ¥")
            return
        }

        guard let targetRec = targetRecognizer else {
            connectionState = .error("ç›®æ¨™èªè¨€è­˜åˆ¥å™¨å‰µå»ºå¤±æ•—")
            _errorSubject.send("ä¸æ”¯æ´ \(targetLang.displayName) èªéŸ³è­˜åˆ¥")
            return
        }

        guard sourceRec.isAvailable else {
            connectionState = .error("ä¾†æºèªè¨€è­˜åˆ¥å™¨ä¸å¯ç”¨")
            _errorSubject.send("è«‹ä¸‹è¼‰ \(sourceLang.displayName) èªè¨€åŒ…")
            return
        }

        guard targetRec.isAvailable else {
            connectionState = .error("ç›®æ¨™èªè¨€è­˜åˆ¥å™¨ä¸å¯ç”¨")
            _errorSubject.send("è«‹ä¸‹è¼‰ \(targetLang.displayName) èªè¨€åŒ…")
            return
        }

        // æª¢æŸ¥è¨­å‚™ç«¯è­˜åˆ¥æ”¯æ´
        let sourceOnDevice = sourceRec.supportsOnDeviceRecognition
        let targetOnDevice = targetRec.supportsOnDeviceRecognition

        print("   ä¾†æºèªè¨€è¨­å‚™ç«¯è­˜åˆ¥: \(sourceOnDevice ? "âœ… æ”¯æ´" : "âŒ ä¸æ”¯æ´")")
        print("   ç›®æ¨™èªè¨€è¨­å‚™ç«¯è­˜åˆ¥: \(targetOnDevice ? "âœ… æ”¯æ´" : "âŒ ä¸æ”¯æ´")")

        // å•Ÿå‹•è­˜åˆ¥ä»»å‹™
        startRecognitionTasks()

        // è¨­ç½®ä»»å‹™é‡å»ºè¨ˆæ™‚å™¨ï¼ˆé¿å… 1 åˆ†é˜é™åˆ¶ï¼‰
        setupTaskRebuildTimer()

        connectionState = .connected
        recognitionStartTime = Date()

        print("âœ… [Apple STT] é›™èªè¨€ä¸¦è¡Œè­˜åˆ¥å·²å•Ÿå‹•")
    }

    private func startRecognitionTasks() {
        guard let sourceRec = sourceRecognizer,
              let targetRec = targetRecognizer else {
            return
        }

        // å‰µå»ºè­˜åˆ¥è«‹æ±‚
        sourceRequest = SFSpeechAudioBufferRecognitionRequest()
        targetRequest = SFSpeechAudioBufferRecognitionRequest()

        guard let sourceReq = sourceRequest,
              let targetReq = targetRequest else {
            connectionState = .error("ç„¡æ³•å‰µå»ºè­˜åˆ¥è«‹æ±‚")
            return
        }

        // é…ç½®è«‹æ±‚
        sourceReq.shouldReportPartialResults = true
        targetReq.shouldReportPartialResults = true

        // â­ï¸ å¼·åˆ¶è¨­å‚™ç«¯è­˜åˆ¥ï¼ˆç„¡ API é…é¡é™åˆ¶ï¼‰
        if sourceRec.supportsOnDeviceRecognition {
            sourceReq.requiresOnDeviceRecognition = true
        }
        if targetRec.supportsOnDeviceRecognition {
            targetReq.requiresOnDeviceRecognition = true
        }

        // æ·»åŠ ä¸Šä¸‹æ–‡æç¤ºï¼ˆå¯é¸ï¼Œæé«˜æº–ç¢ºåº¦ï¼‰
        // sourceReq.contextualStrings = ["å¸¸ç”¨è©å½™"]

        // å•Ÿå‹•ä¾†æºèªè¨€è­˜åˆ¥ä»»å‹™
        sourceTask = sourceRec.recognitionTask(with: sourceReq) { [weak self] result, error in
            self?.handleRecognitionResult(
                result: result,
                error: error,
                isSource: true
            )
        }

        // å•Ÿå‹•ç›®æ¨™èªè¨€è­˜åˆ¥ä»»å‹™
        targetTask = targetRec.recognitionTask(with: targetReq) { [weak self] result, error in
            self?.handleRecognitionResult(
                result: result,
                error: error,
                isSource: false
            )
        }

        print("ğŸ™ï¸ [Apple STT] è­˜åˆ¥ä»»å‹™å·²å•Ÿå‹•")
    }

    private func setupTaskRebuildTimer() {
        taskRebuildTimer?.invalidate()
        taskRebuildTimer = Timer.scheduledTimer(
            withTimeInterval: taskRebuildInterval,
            repeats: true
        ) { [weak self] _ in
            self?.rebuildRecognitionTasks()
        }
    }

    /// é‡å»ºè­˜åˆ¥ä»»å‹™ï¼ˆé¿å… 1 åˆ†é˜è¶…æ™‚é™åˆ¶ï¼‰
    private func rebuildRecognitionTasks() {
        print("ğŸ”„ [Apple STT] é‡å»ºè­˜åˆ¥ä»»å‹™ï¼ˆé¿å…è¶…æ™‚ï¼‰")

        // ç™¼é€ç•¶å‰çµæœ
        emitBestResult(forceFinal: true)

        // çµæŸèˆŠä»»å‹™
        sourceTask?.cancel()
        targetTask?.cancel()
        sourceRequest?.endAudio()
        targetRequest?.endAudio()

        // é‡ç½®çµæœ
        lastSourceResult = nil
        lastTargetResult = nil

        // å•Ÿå‹•æ–°ä»»å‹™
        startRecognitionTasks()
    }

    // MARK: - Recognition Result Handling

    /// éŒ¯èª¤é‡è©¦è¨ˆæ•¸
    private var sourceErrorCount = 0
    private var targetErrorCount = 0
    private let maxErrorRetries = 3

    private func handleRecognitionResult(
        result: SFSpeechRecognitionResult?,
        error: Error?,
        isSource: Bool
    ) {
        let langName = isSource ? sourceLang.shortName : targetLang.shortName
        let langCode = isSource ? sourceLang.rawValue : targetLang.rawValue

        // è™•ç†éŒ¯èª¤
        if let error = error {
            let nsError = error as NSError

            // å¿½ç•¥å–æ¶ˆéŒ¯èª¤
            if nsError.code == 1 || nsError.code == 216 {
                // 1: kAFAssistantErrorDomain - ç”¨æˆ¶å–æ¶ˆ
                // 216: è­˜åˆ¥è¢«ä¸­æ–·
                return
            }

            // â­ï¸ è™•ç† "No speech detected" ç­‰å¯æ¢å¾©éŒ¯èª¤
            let errorMessage = error.localizedDescription
            print("âš ï¸ [Apple STT/\(langName)] éŒ¯èª¤: \(errorMessage) (code: \(nsError.code))")

            // â­ï¸ Error 1110 "No speech detected" æ˜¯æ­£å¸¸çš„
            // è¡¨ç¤ºè­˜åˆ¥å™¨åœ¨é‹è¡Œï¼Œåªæ˜¯æ²’æª¢æ¸¬åˆ°èªéŸ³
            // **ä¸è¦** å› ç‚ºé€™å€‹éŒ¯èª¤è€Œé‡å•Ÿä»»å‹™ï¼
            if nsError.code == 1110 {
                // è¿½è¹¤é€£çºŒéŒ¯èª¤
                consecutiveErrorCount += 1
                if consecutiveErrorCount == 1 {
                    print("â„¹ï¸ [Apple STT] No speech detected - ç­‰å¾…èªéŸ³è¼¸å…¥...")
                } else if consecutiveErrorCount % 10 == 0 {
                    print("â„¹ï¸ [Apple STT] æŒçºŒç­‰å¾…èªéŸ³... (å·²ç­‰å¾… \(consecutiveErrorCount) æ¬¡)")
                }

                // å¦‚æœé€£çºŒå¤ªå¤šæ¬¡æ²’æª¢æ¸¬åˆ°èªéŸ³ï¼Œå¯èƒ½éœ€è¦é‡å»ºä»»å‹™
                if consecutiveErrorCount >= maxConsecutiveErrors * 2 {
                    // ä½†è¦æª¢æŸ¥å†·å»æ™‚é–“
                    if let lastRebuild = lastRebuildTime,
                       Date().timeIntervalSince(lastRebuild) < rebuildCooldown {
                        // é‚„åœ¨å†·å»ä¸­ï¼Œä¸é‡å»º
                        return
                    }

                    print("ğŸ”„ [Apple STT] é•·æ™‚é–“ç„¡èªéŸ³ï¼Œå˜—è©¦é‡å»ºä»»å‹™...")
                    consecutiveErrorCount = 0
                    lastRebuildTime = Date()
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
                        self?.rebuildRecognitionTasks()
                    }
                }
                return
            }

            // è¿½è¹¤å…¶ä»–éŒ¯èª¤æ¬¡æ•¸
            if isSource {
                sourceErrorCount += 1
            } else {
                targetErrorCount += 1
            }

            // â­ï¸ åªæœ‰é 1110 éŒ¯èª¤æ‰è€ƒæ…®é‡å•Ÿ
            if sourceErrorCount > 0 && targetErrorCount > 0 {
                // æª¢æŸ¥å†·å»æ™‚é–“
                if let lastRebuild = lastRebuildTime,
                   Date().timeIntervalSince(lastRebuild) < rebuildCooldown {
                    print("â³ [Apple STT] é‡å»ºå†·å»ä¸­ï¼Œè·³é...")
                    return
                }

                if sourceErrorCount + targetErrorCount <= maxErrorRetries * 2 {
                    print("ğŸ”„ [Apple STT] è­˜åˆ¥å™¨å‡ºéŒ¯ï¼Œå˜—è©¦é‡å•Ÿä»»å‹™...")
                    lastRebuildTime = Date()
                    sourceErrorCount = 0
                    targetErrorCount = 0
                    DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) { [weak self] in
                        self?.rebuildRecognitionTasks()
                    }
                } else {
                    print("âŒ [Apple STT] éŒ¯èª¤æ¬¡æ•¸éå¤šï¼Œåœæ­¢é‡è©¦")
                }
            }
            return
        }

        // æ”¶åˆ°æœ‰æ•ˆçµæœï¼Œé‡ç½®æ‰€æœ‰éŒ¯èª¤è¨ˆæ•¸
        consecutiveErrorCount = 0
        if isSource {
            sourceErrorCount = 0
        } else {
            targetErrorCount = 0
        }

        guard let result = result else { return }

        let text = result.bestTranscription.formattedString
        guard !text.isEmpty else { return }

        // è¨ˆç®—ä¿¡å¿ƒåº¦ï¼ˆä½¿ç”¨æœ€å¾Œä¸€å€‹ segment çš„ä¿¡å¿ƒåº¦ï¼‰
        let confidence = result.bestTranscription.segments.last?.confidence ?? 0
        let isFinal = result.isFinal

        // å‰µå»ºçµæœ
        let recognitionResult = RecognitionResult(
            text: text,
            confidence: confidence,
            language: langCode,
            isFinal: isFinal,
            timestamp: Date()
        )

        // æ›´æ–°å°æ‡‰èªè¨€çš„çµæœ
        if isSource {
            lastSourceResult = recognitionResult
        } else {
            lastTargetResult = recognitionResult
        }

        // â­ï¸ è¨ˆç®—è¾¨è­˜å»¶é²
        var latencyInfo = ""
        if let lastSend = lastAudioSendTime {
            let latency = Date().timeIntervalSince(lastSend) * 1000  // ms
            recognitionLatencies.append(latency)
            if recognitionLatencies.count > 20 {
                recognitionLatencies.removeFirst()
            }
            let avgLatency = recognitionLatencies.reduce(0, +) / Double(recognitionLatencies.count)
            latencyInfo = " | å»¶é²: \(String(format: "%.0f", latency))ms (å¹³å‡: \(String(format: "%.0f", avgLatency))ms)"
        }

        // â­ï¸ Interim çµæœæ²’æœ‰ä¿¡å¿ƒåº¦ï¼ˆApple çš„è¨­è¨ˆï¼‰ï¼Œåªæœ‰ Final æ‰é¡¯ç¤º
        let confidenceInfo = isFinal ? " (ä¿¡å¿ƒ: \(String(format: "%.2f", confidence)))" : ""
        let finalTag = isFinal ? "âœ… Final" : "â³ Interim"
        print("ğŸ¤ [Apple STT/\(langName)] \(finalTag): \"\(text.prefix(40))\"\(confidenceInfo)\(latencyInfo)")

        // é˜²æŠ–è™•ç†ï¼šåˆä½µçŸ­æ™‚é–“å…§çš„çµæœ
        scheduleResultEmission(isFinal: isFinal)
    }

    private func scheduleResultEmission(isFinal: Bool) {
        debounceTimer?.invalidate()

        if isFinal {
            // Final çµæœç«‹å³ç™¼é€
            emitBestResult(forceFinal: true)
        } else {
            // Interim çµæœé˜²æŠ–
            debounceTimer = Timer.scheduledTimer(
                withTimeInterval: debounceInterval,
                repeats: false
            ) { [weak self] _ in
                self?.emitBestResult(forceFinal: false)
            }
        }
    }

    /// æ ¹æ“šä¿¡å¿ƒåº¦é¸æ“‡æœ€ä½³çµæœä¸¦ç™¼é€
    private func emitBestResult(forceFinal: Bool) {
        // ç²å–å…©å€‹è­˜åˆ¥å™¨çš„çµæœ
        let sourceResult = lastSourceResult
        let targetResult = lastTargetResult

        // é¸æ“‡æœ€ä½³çµæœ
        guard let bestResult = selectBestResult(
            source: sourceResult,
            target: targetResult
        ) else {
            return
        }

        // å»é‡ï¼šé¿å…é‡è¤‡ç™¼é€ç›¸åŒå…§å®¹
        if bestResult.text == lastEmittedText && bestResult.language == lastEmittedLanguage && !forceFinal {
            return
        }

        let isFinal = forceFinal || bestResult.isFinal

        // æ›´æ–°å»é‡è¨˜éŒ„
        if isFinal {
            lastEmittedText = ""
            lastEmittedLanguage = ""
        } else {
            lastEmittedText = bestResult.text
            lastEmittedLanguage = bestResult.language
        }

        // å‰µå»º TranscriptMessage
        let transcript = TranscriptMessage(
            text: bestResult.text,
            isFinal: isFinal,
            confidence: Double(bestResult.confidence),
            language: bestResult.language
        )

        // ç™¼é€åˆ°ä¸»ç·šç¨‹
        DispatchQueue.main.async {
            self._transcriptSubject.send(transcript)
        }

        // Final çµæœè§¸ç™¼ç¿»è­¯
        if isFinal && !bestResult.text.isEmpty {
            translateText(text: bestResult.text, detectedLang: bestResult.language)

            // é‡ç½®çµæœ
            lastSourceResult = nil
            lastTargetResult = nil
        }
    }

    /// é¸æ“‡ä¿¡å¿ƒåº¦æœ€é«˜çš„çµæœ
    private func selectBestResult(
        source: RecognitionResult?,
        target: RecognitionResult?
    ) -> RecognitionResult? {

        // â­ï¸ åŒæ™‚é¡¯ç¤ºå…©å€‹è­˜åˆ¥çµæœï¼ˆæ–¹ä¾¿æ¯”è¼ƒï¼‰
        print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        if let s = source {
            print("â”‚ ğŸ“— [\(sourceLang.shortName)] \"\(s.text.prefix(30))\" (ä¿¡å¿ƒ: \(String(format: "%.2f", s.confidence)))")
        } else {
            print("â”‚ ğŸ“— [\(sourceLang.shortName)] (ç„¡çµæœ)")
        }
        if let t = target {
            print("â”‚ ğŸ“˜ [\(targetLang.shortName)] \"\(t.text.prefix(30))\" (ä¿¡å¿ƒ: \(String(format: "%.2f", t.confidence)))")
        } else {
            print("â”‚ ğŸ“˜ [\(targetLang.shortName)] (ç„¡çµæœ)")
        }

        // åªæœ‰ä¸€å€‹çµæœæˆ–éƒ½æ²’æœ‰
        guard let source = source else {
            if target != nil {
                print("â””â”€â†’ é¸æ“‡ \(targetLang.shortName)ï¼ˆåƒ…æœ‰æ­¤çµæœï¼‰")
            } else {
                print("â””â”€â†’ å…©å€‹è­˜åˆ¥å™¨éƒ½ç„¡çµæœ")
            }
            return target
        }
        guard let target = target else {
            print("â””â”€â†’ é¸æ“‡ \(sourceLang.shortName)ï¼ˆåƒ…æœ‰æ­¤çµæœï¼‰")
            return source
        }

        // ä¿¡å¿ƒåº¦å·®ç•°é–¾å€¼ï¼ˆé¿å…å¾®å°å·®ç•°å°è‡´é »ç¹åˆ‡æ›ï¼‰
        let threshold: Float = 0.15

        // æ™‚é–“å·®ç•°é–¾å€¼ï¼ˆåªæ¯”è¼ƒè¿‘æœŸçµæœï¼‰
        let timeThreshold: TimeInterval = 0.5
        let timeDiff = abs(source.timestamp.timeIntervalSince(target.timestamp))

        // å¦‚æœæ™‚é–“å·®å¤ªå¤§ï¼Œé¸æ“‡è¼ƒæ–°çš„
        if timeDiff > timeThreshold {
            let winner = source.timestamp > target.timestamp ? source : target
            let winnerLang = winner.language == sourceLang.rawValue ? sourceLang.shortName : targetLang.shortName
            print("â””â”€â†’ â±ï¸ æ™‚é–“å·®éå¤§ï¼Œé¸æ“‡è¼ƒæ–°: \(winnerLang)")
            return winner
        }

        // æ¯”è¼ƒä¿¡å¿ƒåº¦
        if source.confidence > target.confidence + threshold {
            print("â””â”€â†’ ğŸ† é¸æ“‡ \(sourceLang.shortName)ï¼ˆä¿¡å¿ƒ \(String(format: "%.2f", source.confidence)) > \(String(format: "%.2f", target.confidence))ï¼‰")
            return source
        } else if target.confidence > source.confidence + threshold {
            print("â””â”€â†’ ğŸ† é¸æ“‡ \(targetLang.shortName)ï¼ˆä¿¡å¿ƒ \(String(format: "%.2f", target.confidence)) > \(String(format: "%.2f", source.confidence))ï¼‰")
            return target
        } else {
            // ä¿¡å¿ƒåº¦ç›¸è¿‘ï¼Œé¸æ“‡æ–‡æœ¬æ›´é•·çš„ï¼ˆé€šå¸¸æ›´å®Œæ•´ï¼‰
            if source.text.count >= target.text.count {
                print("â””â”€â†’ ğŸ“ ä¿¡å¿ƒåº¦ç›¸è¿‘ï¼Œé¸æ“‡è¼ƒé•·: \(sourceLang.shortName)ï¼ˆ\(source.text.count) å­—ï¼‰")
                return source
            } else {
                print("â””â”€â†’ ğŸ“ ä¿¡å¿ƒåº¦ç›¸è¿‘ï¼Œé¸æ“‡è¼ƒé•·: \(targetLang.shortName)ï¼ˆ\(target.text.count) å­—ï¼‰")
                return target
            }
        }
    }

    // MARK: - Translation

    private func translateText(text: String, detectedLang: String) {
        Task {
            await callTranslationAPI(text: text, detectedLang: detectedLang)
        }
    }

    private func callTranslationAPI(text: String, detectedLang: String) async {
        // ç¢ºå®šç¿»è­¯æ–¹å‘
        let isSourceLang = detectedLang == sourceLang.rawValue
        let translateTo = isSourceLang ? targetLang.rawValue : sourceLang.rawValue

        print("ğŸŒ [Apple STT] ç¿»è­¯: \(detectedLang) â†’ \(translateTo)")

        // æ§‹å»º API URL
        let urlString = "https://\(serverURL)/smart-translate"
        guard let url = URL(string: urlString) else {
            print("âŒ [Apple STT] ç„¡æ•ˆçš„ç¿»è­¯ URL")
            return
        }

        // æ§‹å»ºè«‹æ±‚
        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")

        let body: [String: Any] = [
            "text": text,
            "sourceLang": sourceLang.rawValue,
            "targetLang": targetLang.rawValue,
            "provider": translationProvider.rawValue
        ]

        do {
            request.httpBody = try JSONSerialization.data(withJSONObject: body)

            let (data, response) = try await URLSession.shared.data(for: request)

            guard let httpResponse = response as? HTTPURLResponse,
                  httpResponse.statusCode == 200 else {
                print("âŒ [Apple STT] ç¿»è­¯ API éŒ¯èª¤")
                return
            }

            // è§£æéŸ¿æ‡‰
            if let json = try JSONSerialization.jsonObject(with: data) as? [String: Any] {
                // â­ï¸ è§£æ LLM usage ä¸¦è¨˜éŒ„è¨ˆè²»ï¼ˆAPI è¿”å›é§å³°å‘½åï¼‰
                if let usage = json["usage"] as? [String: Any] {
                    let inputTokens = usage["inputTokens"] as? Int ?? 0
                    let outputTokens = usage["outputTokens"] as? Int ?? 0

                    if inputTokens > 0 || outputTokens > 0 {
                        BillingService.shared.recordLLMUsage(
                            inputTokens: inputTokens,
                            outputTokens: outputTokens,
                            provider: translationProvider
                        )
                        print("ğŸ’° [Apple STT] LLM è¨ˆè²»: \(inputTokens) + \(outputTokens) tokens")
                    }
                }

                // å˜—è©¦è§£æ segments
                if let segmentsArray = json["segments"] as? [[String: Any]] {
                    var segments: [TranslationSegment] = []
                    for seg in segmentsArray {
                        if let original = seg["original"] as? String,
                           let translation = seg["translation"] as? String {
                            let isComplete = seg["isComplete"] as? Bool ?? true
                            segments.append(TranslationSegment(
                                original: original,
                                translation: translation,
                                isComplete: isComplete
                            ))
                        }
                    }

                    if !segments.isEmpty {
                        let fullTranslation = segments.map { $0.translation }.joined(separator: " ")

                        DispatchQueue.main.async {
                            self._segmentedTranslationSubject.send((text, segments))
                            self._translationSubject.send((text, fullTranslation))
                        }

                        print("âœ… [Apple STT] ç¿»è­¯å®Œæˆ: \"\(fullTranslation.prefix(40))...\"")
                        return
                    }
                }

                // å›é€€ï¼šä½¿ç”¨ç°¡å–®ç¿»è­¯
                if let translation = json["translation"] as? String {
                    DispatchQueue.main.async {
                        self._translationSubject.send((text, translation))
                    }
                    print("âœ… [Apple STT] ç¿»è­¯å®Œæˆ: \"\(translation.prefix(40))...\"")
                }
            }

        } catch {
            print("âŒ [Apple STT] ç¿»è­¯è«‹æ±‚å¤±æ•—: \(error.localizedDescription)")
        }
    }

    // MARK: - Audio Conversion

    /// å°‡ PCM Int16 Data è½‰æ›ç‚º AVAudioPCMBuffer
    private func convertToAudioBuffer(data: Data) -> AVAudioPCMBuffer? {
        // è¨ˆç®—å¹€æ•¸ï¼ˆ16-bit = 2 bytes per sampleï¼‰
        let frameCount = UInt32(data.count) / 2
        guard frameCount > 0 else { return nil }

        // å‰µå»º buffer
        guard let buffer = AVAudioPCMBuffer(
            pcmFormat: audioFormat,
            frameCapacity: frameCount
        ) else {
            return nil
        }
        buffer.frameLength = frameCount

        // è½‰æ› Int16 â†’ Float32
        data.withUnsafeBytes { (rawPtr: UnsafeRawBufferPointer) in
            guard let int16Ptr = rawPtr.baseAddress?.assumingMemoryBound(to: Int16.self),
                  let floatChannelData = buffer.floatChannelData else {
                return
            }

            let floatPtr = floatChannelData[0]
            for i in 0..<Int(frameCount) {
                floatPtr[i] = Float(int16Ptr[i]) / Float(Int16.max)
            }
        }

        return buffer
    }

    // MARK: - â­ï¸ è‡ªå‹•èªè¨€åˆ‡æ›ï¼ˆç¶“æ¿Ÿæ¨¡å¼ï¼‰

    /// é–‹å§‹èªè¨€æ¯”è¼ƒæµç¨‹
    /// åˆ‡æ›åˆ°å¦ä¸€ç¨®èªè¨€ï¼Œç”¨ç·©è¡å€éŸ³é »é‡æ–°è­˜åˆ¥
    private func startLanguageComparison() {
        guard audioRingBuffer.hasData else {
            print("âš ï¸ [è‡ªå‹•åˆ‡æ›] ç·©è¡å€ç„¡æ•¸æ“šï¼Œè·³éæ¯”è¼ƒ")
            finalizeComparison()
            return
        }

        isComparingLanguages = true
        let originalLanguage = currentActiveLanguage
        let otherLanguage = (currentActiveLanguage == sourceLang) ? targetLang : sourceLang

        print("ğŸ”„ [è‡ªå‹•åˆ‡æ›] åˆ‡æ›åˆ° \(otherLanguage.shortName) é€²è¡Œæ¯”è¼ƒ...")
        print("   ğŸ“¼ ä½¿ç”¨ç·©è¡å€ \(String(format: "%.1f", audioRingBuffer.bufferedDuration)) ç§’éŸ³é »")

        // ç²å–ç·©è¡å€éŸ³é »
        let bufferedAudio = audioRingBuffer.readAll()

        // åœæ­¢ç•¶å‰è­˜åˆ¥
        stopSingleLanguageRecognition()

        // åˆ‡æ›èªè¨€
        currentActiveLanguage = otherLanguage

        // å‰µå»ºæ–°çš„è­˜åˆ¥ä»»å‹™
        startSingleLanguageRecognition()

        // â­ï¸ é‡è¦ï¼šç­‰å¾…è­˜åˆ¥å™¨å®Œå…¨æº–å‚™å¥½å†ç™¼é€éŸ³é »
        // startSingleLanguageRecognition() æ˜¯ç•°æ­¥çš„ï¼Œéœ€è¦è¶³å¤ æ™‚é–“è®“ï¼š
        // 1. SFSpeechRecognizer åˆå§‹åŒ–
        // 2. recognitionTask å‰µå»ºä¸¦å•Ÿå‹•
        // 3. å›èª¿ç¶å®šå®Œæˆ
        // 0.1 ç§’ä¸å¤ ï¼Œæ”¹ç‚º 0.5 ç§’
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
            guard let self = self, self.isComparingLanguages else { return }
            print("âœ… [è‡ªå‹•åˆ‡æ›] è­˜åˆ¥å™¨æº–å‚™å®Œæˆï¼Œé–‹å§‹ç™¼é€ç·©è¡éŸ³é »")
            self.resendBufferedAudio(bufferedAudio)
        }
    }

    /// é‡æ–°ç™¼é€ç·©è¡å€éŸ³é »
    private func resendBufferedAudio(_ data: Data) {
        guard let buffer = convertToAudioBuffer(data: data) else {
            print("âŒ [è‡ªå‹•åˆ‡æ›] ç·©è¡å€éŸ³é »è½‰æ›å¤±æ•—")
            finalizeComparison()
            return
        }

        let audioDuration = Double(data.count) / Double(16000 * 2)  // 16kHz, 16-bit
        print("ğŸ“¤ [è‡ªå‹•åˆ‡æ›] é‡æ–°ç™¼é€ \(data.count) bytes ç·©è¡éŸ³é » (ç´„ \(String(format: "%.1f", audioDuration))ç§’)")

        // ç™¼é€åˆ°è­˜åˆ¥å™¨
        if let request = sourceRequest {
            request.append(buffer)
            print("âœ… [è‡ªå‹•åˆ‡æ›] éŸ³é »å·²ç™¼é€åˆ°è­˜åˆ¥å™¨")

            // â­ï¸ ç«‹å³èª¿ç”¨ endAudio() è§¸ç™¼ Final
            print("ğŸ”š [è‡ªå‹•åˆ‡æ›] èª¿ç”¨ endAudio() è§¸ç™¼ Final")
            request.endAudio()
        } else {
            print("âŒ [è‡ªå‹•åˆ‡æ›] sourceRequest ç‚º nilï¼Œç„¡æ³•ç™¼é€éŸ³é »")
            finalizeComparison()
            return
        }

        // è¨­ç½®è¶…æ™‚ï¼ˆç­‰å¾… Final çµæœï¼Œæœ€å¤š 5 ç§’ï¼‰
        DispatchQueue.main.asyncAfter(deadline: .now() + 5.0) { [weak self] in
            guard let self = self, self.isComparingLanguages else { return }
            print("â±ï¸ [è‡ªå‹•åˆ‡æ›] ç­‰å¾… Final è¶…æ™‚ï¼Œä½¿ç”¨ç¾æœ‰çµæœ")
            self.finalizeComparison()
        }
    }

    /// è™•ç†æ¯”è¼ƒæ¨¡å¼ä¸‹çš„è­˜åˆ¥çµæœ
    private func handleComparisonResult(text: String, confidence: Float, isFinal: Bool) {
        let tag = isFinal ? "Final" : "Interim"
        print("ğŸ“Š [è‡ªå‹•åˆ‡æ›] æ”¶åˆ°æ¯”è¼ƒçµæœ (\(tag)): \(currentActiveLanguage.shortName) = \"\(text.prefix(30))\" (ä¿¡å¿ƒ: \(String(format: "%.2f", confidence)))")

        // å„²å­˜çµæœï¼ˆFinal çµæœè¦†è“‹ Interimï¼‰
        comparisonResults[currentActiveLanguage] = (text: text, confidence: confidence)

        // åªæœ‰ Final çµæœæ‰çµæŸæ¯”è¼ƒ
        if isFinal {
            finalizeComparison()
        }
    }

    /// å®Œæˆæ¯”è¼ƒï¼Œé¸æ“‡æœ€ä½³çµæœ
    private func finalizeComparison() {
        isComparingLanguages = false

        // æ¯”è¼ƒçµæœ
        print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print("â”‚ ğŸ“Š [æ¯”è¼ƒ] èªè¨€æ¯”è¼ƒçµæœ:")

        // â­ï¸ æ¯”è¼ƒé¡¯ç¤ºæ¨¡å¼ï¼šç™¼é€æ‰€æœ‰çµæœè®“ UI é¡¯ç¤º
        if isComparisonDisplayMode {
            var allResults: [(lang: Language, text: String, confidence: Float, isFinal: Bool)] = []

            for (lang, result) in comparisonResults {
                let isFinal = result.confidence >= 0
                let displayConfidence = result.confidence < 0 ? "N/A" : String(format: "%.2f", result.confidence)
                let tag = isFinal ? "Final" : "Interim"
                print("â”‚   \(lang.shortName): \"\(result.text.prefix(30))\" (\(tag), ä¿¡å¿ƒ: \(displayConfidence))")

                allResults.append((
                    lang: lang,
                    text: result.text,
                    confidence: result.confidence < 0 ? 0 : result.confidence,
                    isFinal: isFinal
                ))
            }

            print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

            // ç™¼é€æ¯”è¼ƒçµæœåˆ° UI
            DispatchQueue.main.async {
                self.onComparisonResults?(allResults)
            }

            // æ¸…ç©ºæ¯”è¼ƒçµæœ
            comparisonResults.removeAll()
            audioRingBuffer.clear()

            // æ¢å¾©åˆ°ä¾†æºèªè¨€
            stopSingleLanguageRecognition()
            currentActiveLanguage = sourceLang
            startSingleLanguageRecognition()
            return
        }

        // â­ï¸ è‡ªå‹•åˆ‡æ›æ¨¡å¼ï¼šé¸æ“‡æœ€ä½³çµæœ
        var bestLanguage: Language = currentActiveLanguage
        var bestText: String = ""
        var bestConfidence: Float = -999  // ç”¨æ–¼æ¯”è¼ƒï¼Œ-1 æ˜¯ Interim æ¨™è¨˜

        for (lang, result) in comparisonResults {
            let isFinal = result.confidence >= 0
            let isReliableFinal = isFinal && result.confidence >= unreliableFinalThreshold
            let tag = isFinal ? (isReliableFinal ? "Final" : "Final(ä½ä¿¡å¿ƒ)") : "Interim"
            let displayConfidence = result.confidence < 0 ? "N/A" : String(format: "%.2f", result.confidence)
            print("â”‚   \(lang.shortName): \"\(result.text.prefix(25))\" (\(tag), ä¿¡å¿ƒ: \(displayConfidence))")

            // â­ï¸ æ”¹é€²çš„æ¯”è¼ƒé‚è¼¯ï¼š
            // 1. å¯é  Final (ä¿¡å¿ƒ >= 0.30) å„ªå…ˆ
            // 2. ä¸å¯é  Final (ä¿¡å¿ƒ < 0.30) å’Œ Interim è¦–ç‚ºåŒç­‰ï¼ŒæŒ‰æ–‡å­—é•·åº¦æ¯”è¼ƒ
            // 3. åŒç‚ºå¯é  Final æ™‚ï¼Œæ¯”è¼ƒä¿¡å¿ƒåº¦

            let currentIsFinal = result.confidence >= 0
            let currentIsReliable = currentIsFinal && result.confidence >= unreliableFinalThreshold
            let bestIsFinal = bestConfidence >= 0
            let bestIsReliable = bestIsFinal && bestConfidence >= unreliableFinalThreshold

            let isBetter: Bool
            if currentIsReliable && !bestIsReliable {
                // æ–°çµæœæ˜¯å¯é  Finalï¼ŒèˆŠçµæœä¸æ˜¯
                isBetter = true
            } else if !currentIsReliable && bestIsReliable {
                // æ–°çµæœä¸å¯é ï¼ŒèˆŠçµæœæ˜¯å¯é  Final
                isBetter = false
            } else if currentIsReliable && bestIsReliable {
                // å…©å€‹éƒ½æ˜¯å¯é  Finalï¼Œæ¯”è¼ƒä¿¡å¿ƒåº¦
                isBetter = result.confidence > bestConfidence
            } else {
                // å…©å€‹éƒ½ä¸å¯é ï¼ˆInterim æˆ–ä½ä¿¡å¿ƒ Finalï¼‰ï¼Œé¸æ“‡æ–‡å­—è¼ƒé•·çš„
                isBetter = result.text.count > bestText.count
            }

            if isBetter {
                bestLanguage = lang
                bestText = result.text
                bestConfidence = result.confidence
            }
        }

        let displayBestConfidence = bestConfidence < 0 ? "N/A" : String(format: "%.2f", bestConfidence)
        print("â”‚ ğŸ† é¸æ“‡: \(bestLanguage.shortName) (ä¿¡å¿ƒ: \(displayBestConfidence))")
        print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

        // å¦‚æœæœ€ä½³èªè¨€ä¸æ˜¯ç•¶å‰èªè¨€ï¼Œåˆ‡æ›
        if bestLanguage != currentActiveLanguage {
            print("ğŸ”„ [è‡ªå‹•åˆ‡æ›] åˆ‡æ›åˆ° \(bestLanguage.shortName)")
            stopSingleLanguageRecognition()
            currentActiveLanguage = bestLanguage
            startSingleLanguageRecognition()
        }

        // â­ï¸ é€šçŸ¥ UI æ›´æ–°ï¼ˆç„¡è«–æ˜¯å¦åˆ‡æ›ï¼Œéƒ½è¦åŒæ­¥ç‹€æ…‹ï¼‰
        DispatchQueue.main.async {
            self.onLanguageSwitched?(bestLanguage)
        }

        // æ¸…ç©ºæ¯”è¼ƒçµæœ
        comparisonResults.removeAll()

        // æ¸…ç©ºç·©è¡å€
        audioRingBuffer.clear()

        // ç™¼é€æœ€ä½³çµæœ
        if !bestText.isEmpty {
            // ä½¿ç”¨æœ€ä½³ä¿¡å¿ƒåº¦ï¼Œå¦‚æœæ˜¯è² æ•¸ï¼ˆInterimï¼‰å‰‡è¨­ç‚º 0.5
            let displayConfidence = bestConfidence < 0 ? 0.5 : Double(bestConfidence)

            let transcript = TranscriptMessage(
                text: bestText,
                isFinal: true,
                confidence: displayConfidence,
                language: bestLanguage.rawValue
            )

            DispatchQueue.main.async {
                self._transcriptSubject.send(transcript)
            }

            // è§¸ç™¼ç¿»è­¯
            translateText(text: bestText, detectedLang: bestLanguage.rawValue)

            // å›èª¿é€šçŸ¥
            onLanguageComparisonComplete?(bestLanguage, bestText, bestConfidence)
        }
    }

    // MARK: - â­ï¸ ç¶“æ¿Ÿæ¨¡å¼é›™èªè¨€æ‰¹é‡æ¯”è¼ƒ

    /// é›™èªè¨€æ¯”è¼ƒçµæœæš«å­˜
    private var dualComparisonResults: [Language: (text: String, confidence: Float, isFinal: Bool)] = [:]
    private var dualComparisonPendingLanguages: Set<Language> = []

    /// æ¸…ç©ºéŸ³é »ç·©è¡å€
    func clearAudioBuffer() {
        audioRingBuffer.clear()
        print("ğŸ—‘ï¸ [Apple STT] éŸ³é »ç·©è¡å€å·²æ¸…ç©º")
    }

    /// é–‹å§‹é›™èªè¨€æ‰¹é‡æ¯”è¼ƒï¼ˆç¶“æ¿Ÿæ¨¡å¼å°ˆç”¨ï¼‰
    /// ç”¨ç·©è¡å€éŸ³é »åˆ†åˆ¥é€çµ¦å…©å€‹èªè¨€çš„è­˜åˆ¥å™¨ï¼Œç­‰å¾…å…©å€‹ Final çµæœ
    func startDualLanguageComparison() {
        guard audioRingBuffer.hasData else {
            print("âš ï¸ [é›™èªè¨€æ¯”è¼ƒ] ç·©è¡å€ç„¡æ•¸æ“š")
            return
        }

        let bufferedAudio = audioRingBuffer.readAll()
        let audioDuration = Double(bufferedAudio.count) / Double(16000 * 2)
        print("ğŸ”¬ [é›™èªè¨€æ¯”è¼ƒ] é–‹å§‹æ¯”è¼ƒï¼ŒéŸ³é »: \(String(format: "%.1f", audioDuration))ç§’")

        // é‡ç½®æ¯”è¼ƒç‹€æ…‹
        dualComparisonResults.removeAll()
        dualComparisonPendingLanguages = [sourceLang, targetLang]

        // åœæ­¢ç•¶å‰è­˜åˆ¥
        stopSingleLanguageRecognition()

        // ä¾åºè­˜åˆ¥å…©ç¨®èªè¨€
        recognizeWithLanguage(sourceLang, audio: bufferedAudio) { [weak self] in
            guard let self = self else { return }
            // ç¬¬ä¸€å€‹èªè¨€å®Œæˆï¼Œé–‹å§‹ç¬¬äºŒå€‹
            self.recognizeWithLanguage(self.targetLang, audio: bufferedAudio) { [weak self] in
                // å…©å€‹éƒ½å®Œæˆ
                self?.finalizeDualComparison()
            }
        }
    }

    /// ç”¨æŒ‡å®šèªè¨€è­˜åˆ¥éŸ³é »
    private func recognizeWithLanguage(_ language: Language, audio: Data, completion: @escaping () -> Void) {
        print("ğŸ¯ [é›™èªè¨€æ¯”è¼ƒ] é–‹å§‹è­˜åˆ¥: \(language.shortName)")

        let locale = Locale(identifier: language.azureLocale)
        guard let recognizer = SFSpeechRecognizer(locale: locale),
              recognizer.isAvailable else {
            print("âŒ [é›™èªè¨€æ¯”è¼ƒ] \(language.shortName) è­˜åˆ¥å™¨ä¸å¯ç”¨")
            dualComparisonResults[language] = (text: "(ä¸æ”¯æ´)", confidence: 0, isFinal: true)
            dualComparisonPendingLanguages.remove(language)
            completion()
            return
        }

        // å‰µå»ºè­˜åˆ¥è«‹æ±‚
        let request = SFSpeechAudioBufferRecognitionRequest()
        request.shouldReportPartialResults = true
        if recognizer.supportsOnDeviceRecognition {
            request.requiresOnDeviceRecognition = true
        }

        // è½‰æ›éŸ³é »
        guard let buffer = convertToAudioBuffer(data: audio) else {
            print("âŒ [é›™èªè¨€æ¯”è¼ƒ] éŸ³é »è½‰æ›å¤±æ•—")
            dualComparisonResults[language] = (text: "(è½‰æ›å¤±æ•—)", confidence: 0, isFinal: true)
            dualComparisonPendingLanguages.remove(language)
            completion()
            return
        }

        // è¿½è¹¤æ˜¯å¦å·²å®Œæˆ
        var hasCompleted = false

        // å•Ÿå‹•è­˜åˆ¥ä»»å‹™
        let task = recognizer.recognitionTask(with: request) { [weak self] result, error in
            guard let self = self else { return }

            if let error = error {
                let nsError = error as NSError
                // å¿½ç•¥å–æ¶ˆéŒ¯èª¤
                if nsError.code != 1 && nsError.code != 216 {
                    print("âš ï¸ [é›™èªè¨€æ¯”è¼ƒ/\(language.shortName)] éŒ¯èª¤: \(error.localizedDescription)")
                }
            }

            if let result = result {
                let text = result.bestTranscription.formattedString
                let confidence = result.bestTranscription.segments.last?.confidence ?? 0
                let isFinal = result.isFinal

                // æ›´æ–°çµæœ
                self.dualComparisonResults[language] = (text: text, confidence: confidence, isFinal: isFinal)

                let tag = isFinal ? "âœ… Final" : "â³ Interim"
                print("ğŸ“Š [é›™èªè¨€æ¯”è¼ƒ/\(language.shortName)] \(tag): \"\(text.prefix(30))\" (ä¿¡å¿ƒ: \(String(format: "%.2f", confidence)))")

                // Final çµæœæ™‚å®Œæˆ
                if isFinal && !hasCompleted {
                    hasCompleted = true
                    self.dualComparisonPendingLanguages.remove(language)
                    completion()
                }
            }
        }

        // ç™¼é€éŸ³é »
        request.append(buffer)
        print("ğŸ“¤ [é›™èªè¨€æ¯”è¼ƒ/\(language.shortName)] å·²ç™¼é€ \(audio.count) bytes")

        // èª¿ç”¨ endAudio è§¸ç™¼ Final
        request.endAudio()
        print("ğŸ”š [é›™èªè¨€æ¯”è¼ƒ/\(language.shortName)] å·²èª¿ç”¨ endAudio()")

        // è¨­ç½®è¶…æ™‚ï¼ˆ5 ç§’ï¼‰
        DispatchQueue.main.asyncAfter(deadline: .now() + 5.0) { [weak self] in
            guard let self = self, !hasCompleted else { return }
            hasCompleted = true
            print("â±ï¸ [é›™èªè¨€æ¯”è¼ƒ/\(language.shortName)] è¶…æ™‚ï¼Œä½¿ç”¨ç¾æœ‰çµæœ")
            task.cancel()
            self.dualComparisonPendingLanguages.remove(language)
            completion()
        }
    }

    /// å®Œæˆé›™èªè¨€æ¯”è¼ƒï¼Œé¸æ“‡æœ€ä½³çµæœä¸¦è§¸ç™¼ç¿»è­¯
    private func finalizeDualComparison() {
        print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print("â”‚ ğŸ”¬ [é›™èªè¨€æ¯”è¼ƒ] çµæœ:")

        // â­ï¸ æ”¶é›†æ‰€æœ‰çµæœä¸¦é¸æ“‡æœ€ä½³
        var allResults: [(lang: Language, text: String, confidence: Float, isFinal: Bool)] = []
        var bestLang: Language = sourceLang
        var bestText: String = ""
        var bestConfidence: Float = -1

        for (lang, result) in dualComparisonResults {
            let tag = result.isFinal ? "Final" : "Interim"
            print("â”‚   \(lang.shortName): \"\(result.text.prefix(30))\" (\(tag), ä¿¡å¿ƒ: \(String(format: "%.2f", result.confidence)))")

            allResults.append((
                lang: lang,
                text: result.text,
                confidence: result.confidence,
                isFinal: result.isFinal
            ))

            // â­ï¸ é¸æ“‡ä¿¡å¿ƒæ°´æº–æœ€é«˜çš„
            // å¦‚æœä¿¡å¿ƒåº¦ç›¸åŒï¼Œé¸æ“‡æ–‡æœ¬è¼ƒé•·çš„ï¼ˆé€šå¸¸æ›´å®Œæ•´ï¼‰
            let isBetter: Bool
            if result.confidence > bestConfidence + 0.05 {
                // ä¿¡å¿ƒåº¦æ˜é¡¯æ›´é«˜
                isBetter = true
            } else if result.confidence < bestConfidence - 0.05 {
                // ä¿¡å¿ƒåº¦æ˜é¡¯æ›´ä½
                isBetter = false
            } else {
                // ä¿¡å¿ƒåº¦ç›¸è¿‘ï¼Œé¸æ“‡æ–‡æœ¬è¼ƒé•·çš„
                isBetter = result.text.count > bestText.count
            }

            if isBetter && !result.text.isEmpty {
                bestLang = lang
                bestText = result.text
                bestConfidence = result.confidence
            }
        }

        print("â”‚ ğŸ† é¸æ“‡: \(bestLang.shortName) (ä¿¡å¿ƒ: \(String(format: "%.2f", bestConfidence)))")
        print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

        // â­ï¸ æ›´æ–°ç•¶å‰æ´»å‹•èªè¨€ï¼ˆä¸‹æ¬¡éŒ„éŸ³é è¨­ç”¨é€™å€‹èªè¨€ï¼‰
        currentActiveLanguage = bestLang

        // æ¸…ç©ºç·©è¡å€
        audioRingBuffer.clear()

        // â­ï¸ å¦‚æœæœ‰æœ‰æ•ˆçµæœï¼Œç™¼é€ä¸¦è§¸ç™¼ç¿»è­¯
        if !bestText.isEmpty {
            // å‰µå»º TranscriptMessage
            let transcript = TranscriptMessage(
                text: bestText,
                isFinal: true,
                confidence: Double(bestConfidence),
                language: bestLang.rawValue
            )

            // ç™¼é€åˆ°ä¸»ç·šç¨‹
            DispatchQueue.main.async {
                self._transcriptSubject.send(transcript)

                // â­ï¸ é€šçŸ¥ ViewModel é¸ä¸­çš„èªè¨€å’Œçµæœ
                self.onBestComparisonResult?(bestLang, bestText, bestConfidence)
            }

            // â­ï¸ è§¸ç™¼ç¿»è­¯ API
            translateText(text: bestText, detectedLang: bestLang.rawValue)
        }

        // æ¢å¾©è­˜åˆ¥å™¨ï¼ˆæº–å‚™ä¸‹ä¸€æ¬¡éŒ„éŸ³ï¼‰
        startSingleLanguageRecognition()
    }

    // MARK: - Static Methods

    /// æª¢æŸ¥èªè¨€æ˜¯å¦æ”¯æ´ Apple STT
    static func isLanguageSupported(_ language: Language) -> Bool {
        let locale = Locale(identifier: language.azureLocale)
        guard let recognizer = SFSpeechRecognizer(locale: locale) else {
            return false
        }
        return recognizer.isAvailable
    }

    /// æª¢æŸ¥èªè¨€æ˜¯å¦æ”¯æ´è¨­å‚™ç«¯è­˜åˆ¥
    static func supportsOnDeviceRecognition(_ language: Language) -> Bool {
        let locale = Locale(identifier: language.azureLocale)
        guard let recognizer = SFSpeechRecognizer(locale: locale) else {
            return false
        }
        return recognizer.supportsOnDeviceRecognition
    }

    /// åˆ—å‡ºæ‰€æœ‰æ”¯æ´çš„èªè¨€
    static func listSupportedLanguages() {
        print("ğŸ“‹ [Apple STT] æ”¯æ´çš„èªè¨€åˆ—è¡¨:")
        for lang in Language.allCases {
            if lang == .auto { continue }
            let supported = isLanguageSupported(lang)
            let onDevice = supportsOnDeviceRecognition(lang)
            let status = supported ? (onDevice ? "âœ… è¨­å‚™ç«¯" : "â˜ï¸ é›²ç«¯") : "âŒ ä¸æ”¯æ´"
            print("   \(lang.displayName): \(status)")
        }
    }
}
