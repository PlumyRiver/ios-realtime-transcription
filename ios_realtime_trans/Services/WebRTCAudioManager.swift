//
//  WebRTCAudioManager.swift
//  ios_realtime_trans
//
//  ä½¿ç”¨ WebRTC AudioEngine æ¨¡å¼çš„å…¨é›™å·¥éŸ³é »ç®¡ç†å™¨
//
//  æ¶æ§‹è¨­è¨ˆï¼š
//  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
//  â”‚  WebRTC RTCAudioDeviceModule (AudioEngine æ¨¡å¼)                 â”‚
//  â”‚                                                                 â”‚
//  â”‚  éº¥å…‹é¢¨ â†’ inputNode â†’ [tapMixer + tap] â†’ WebRTC å…§éƒ¨è™•ç†        â”‚
//  â”‚                              â†“                                  â”‚
//  â”‚                        PCM æ•¸æ“š â†’ WebSocket                     â”‚
//  â”‚                                                                 â”‚
//  â”‚  TTS æ’­æ”¾ â†’ WebRTC outputNode â†’ æšè²å™¨                          â”‚
//  â”‚                                                                 â”‚
//  â”‚  â­ï¸ å…¨éƒ¨ä½¿ç”¨ WebRTC çš„ AudioEngineï¼ŒAEC3 è‡ªå‹•è™•ç†å›éŸ³          â”‚
//  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
//

import Foundation
import AVFoundation
import AVFAudio
import Combine
import WebRTC

// MARK: - Recording State

enum WebRTCRecordingState: Equatable {
    case idle
    case recording
    case error(Error)

    static func == (lhs: WebRTCRecordingState, rhs: WebRTCRecordingState) -> Bool {
        switch (lhs, rhs) {
        case (.idle, .idle): return true
        case (.recording, .recording): return true
        case (.error, .error): return true
        default: return false
        }
    }
}

// MARK: - Recording Error

enum WebRTCRecordingError: Error, LocalizedError {
    case permissionDenied
    case invalidFormat
    case engineStartFailed
    case webrtcInitFailed

    var errorDescription: String? {
        switch self {
        case .permissionDenied: return "éº¥å…‹é¢¨æ¬Šé™è¢«æ‹’çµ•"
        case .invalidFormat: return "éŸ³é »æ ¼å¼ç„¡æ•ˆ"
        case .engineStartFailed: return "éŸ³é »å¼•æ“å•Ÿå‹•å¤±æ•—"
        case .webrtcInitFailed: return "WebRTC åˆå§‹åŒ–å¤±æ•—"
        }
    }
}

// MARK: - TTS Error

enum WebRTCTTSError: Error, LocalizedError {
    case audioFileError
    case playbackFailed
    case engineNotReady

    var errorDescription: String? {
        switch self {
        case .audioFileError: return "éŸ³é »æ–‡ä»¶éŒ¯èª¤"
        case .playbackFailed: return "æ’­æ”¾å¤±æ•—"
        case .engineNotReady: return "éŸ³é »å¼•æ“æœªæº–å‚™å¥½"
        }
    }
}

// MARK: - WebRTC Audio Manager

/// WebRTC AudioEngine æ¨¡å¼å…¨é›™å·¥éŸ³é »ç®¡ç†å™¨
@Observable
final class WebRTCAudioManager: NSObject {

    // MARK: - Singleton

    static let shared = WebRTCAudioManager()

    // MARK: - Public Properties

    /// éŒ„éŸ³ç‹€æ…‹
    private(set) var recordingState: WebRTCRecordingState = .idle

    /// TTS æ’­æ”¾ç‹€æ…‹
    private(set) var isPlayingTTS: Bool = false

    /// ç•¶å‰æ’­æ”¾çš„ TTS æ–‡æœ¬
    private(set) var currentTTSText: String?

    /// Push-to-Talk æ¨¡å¼
    private(set) var isManualSendingPaused: Bool = true

    /// æ“´éŸ³æ¨¡å¼
    var isSpeakerMode: Bool = true {
        didSet {
            if oldValue != isSpeakerMode {
                updateOutputRoute()
            }
        }
    }

    /// éŸ³é‡å¢ç›Šï¼ˆdBï¼‰
    static let maxVolumeDB: Float = 36.0
    var volumeBoostDB: Float = 24.0 {
        didSet {
            updateVolumeGain()
        }
    }

    /// éŸ³é‡ç™¾åˆ†æ¯”
    var volumePercent: Float {
        get { volumeBoostDB / Self.maxVolumeDB }
        set {
            let clamped = min(max(newValue, 0), 1)
            volumeBoostDB = clamped * Self.maxVolumeDB
        }
    }

    // MARK: - WebRTC Components

    /// PeerConnection Factory
    private var factory: RTCPeerConnectionFactory!

    /// AudioDeviceModule
    private var audioDeviceModule: RTCAudioDeviceModule!

    /// WebRTC ç®¡ç†çš„ AVAudioEngineï¼ˆé€šé delegate ç²å–ï¼‰
    private var webrtcEngine: AVAudioEngine?

    /// æœ¬åœ°éŸ³é »è»Œé“
    private var localAudioTrack: RTCAudioTrack?

    /// éŸ³é »æº
    private var audioSource: RTCAudioSource?

    // MARK: - Audio Tapï¼ˆåœ¨ WebRTC Engine ä¸­æ•ç²éŸ³é »ï¼‰

    /// ç”¨æ–¼æ•ç²è¼¸å…¥éŸ³é »çš„ Mixer ç¯€é»
    private var tapMixerNode: AVAudioMixerNode?

    /// éŸ³é »æ ¼å¼è½‰æ›å™¨
    private var audioConverter: AVAudioConverter?

    /// è¼¸å‡ºæ ¼å¼ï¼ˆ16kHz mono 16-bitï¼‰
    private var outputFormat: AVAudioFormat?

    // MARK: - TTS Playbackï¼ˆä½¿ç”¨ WebRTC Engine æ’­æ”¾ï¼‰

    /// TTS æ’­æ”¾å™¨ç¯€é»ï¼ˆé€£æ¥åˆ° WebRTC Engineï¼‰
    private var ttsPlayerNode: AVAudioPlayerNode?

    /// TTS EQ ç¯€é»
    private var ttsEQNode: AVAudioUnitEQ?

    /// TTS éŸ³é »æ–‡ä»¶
    private var ttsAudioFile: AVAudioFile?

    /// æ’­æ”¾ç›£æ§å®šæ™‚å™¨
    private var playbackTimer: Timer?

    /// é˜²æ­¢é‡è¤‡è§¸ç™¼å®Œæˆå›èª¿
    private var hasTriggeredCompletion: Bool = false

    /// TTS ç¯€é»æ˜¯å¦å·²é€£æ¥
    private var ttsNodesConnected: Bool = false

    // MARK: - Audio Buffer

    private var audioBufferCollector: [Data] = []
    private var bufferTimer: Timer?
    private let bufferInterval: TimeInterval = 0.25
    private var sendCount = 0
    private let maxChunkSize = 25600

    // MARK: - Combine Publishers

    private let audioDataSubject = PassthroughSubject<Data, Never>()

    var audioDataPublisher: AnyPublisher<Data, Never> {
        audioDataSubject.eraseToAnyPublisher()
    }

    /// TTS æ’­æ”¾å®Œæˆå›èª¿
    var onTTSPlaybackFinished: (() -> Void)?

    /// PTT çµæŸèªå¥å›èª¿
    var onEndUtterance: (() -> Void)?

    // MARK: - Initialization

    private override init() {
        super.init()
        setupWebRTC()
    }

    // MARK: - WebRTC Setup

    /// è¨­ç½® WebRTC
    private func setupWebRTC() {
        RTCInitializeSSL()

        let videoEncoderFactory = RTCDefaultVideoEncoderFactory()
        let videoDecoderFactory = RTCDefaultVideoDecoderFactory()

        // â­ï¸ ä½¿ç”¨ AudioEngine æ¨¡å¼ï¼Œå•Ÿç”¨ Voice Processingï¼ˆAECï¼‰
        factory = RTCPeerConnectionFactory(
            audioDeviceModuleType: .audioEngine,
            bypassVoiceProcessing: false,
            encoderFactory: videoEncoderFactory,
            decoderFactory: videoDecoderFactory,
            audioProcessingModule: nil
        )

        // ç²å– AudioDeviceModule ä¸¦è¨­ç½® delegate
        audioDeviceModule = factory.audioDeviceModule
        audioDeviceModule.observer = self

        // å‰µå»ºè¼¸å‡ºæ ¼å¼
        outputFormat = AVAudioFormat(
            commonFormat: .pcmFormatInt16,
            sampleRate: 16000,
            channels: 1,
            interleaved: true
        )

        print("âœ… [WebRTC] Factory åˆå§‹åŒ–å®Œæˆ")
        print("   æ¨¡å¼: AudioEngine")
        print("   Voice Processing: å•Ÿç”¨ï¼ˆAEC å›éŸ³æ¶ˆé™¤ï¼‰")
        print("   Delegate: å·²è¨­ç½®")
    }

    /// æ›´æ–°è¼¸å‡ºè·¯ç”±
    private func updateOutputRoute() {
        let rtcAudioSession = RTCAudioSession.sharedInstance()
        rtcAudioSession.lockForConfiguration()
        do {
            if isSpeakerMode {
                try rtcAudioSession.overrideOutputAudioPort(.speaker)
                print("ğŸ“¢ [WebRTC] æ“´éŸ³æ¨¡å¼ï¼šæšè²å™¨")
            } else {
                try rtcAudioSession.overrideOutputAudioPort(.none)
                print("ğŸ“± [WebRTC] è½ç­’æ¨¡å¼")
            }
        } catch {
            print("âŒ [WebRTC] æ›´æ–°è¼¸å‡ºè·¯ç”±å¤±æ•—: \(error)")
        }
        rtcAudioSession.unlockForConfiguration()
    }

    // MARK: - Voice Isolation

    /// é¡¯ç¤ºç³»çµ±éº¥å…‹é¢¨æ¨¡å¼é¸æ“‡å™¨ï¼ˆVoice Isolationã€Wide Spectrumã€Standardï¼‰
    /// éœ€è¦åœ¨éº¥å…‹é¢¨æ­£åœ¨ä½¿ç”¨æ™‚èª¿ç”¨
    func showMicrophoneModeSelector() {
        AVCaptureDevice.showSystemUserInterface(.microphoneModes)
        print("ğŸ¤ [WebRTC] é¡¯ç¤ºéº¥å…‹é¢¨æ¨¡å¼é¸æ“‡å™¨")
    }

    /// ç²å–ç•¶å‰åå¥½çš„éº¥å…‹é¢¨æ¨¡å¼
    var preferredMicrophoneMode: AVCaptureDevice.MicrophoneMode {
        AVCaptureDevice.preferredMicrophoneMode
    }

    /// ç²å–ç•¶å‰å•Ÿç”¨çš„éº¥å…‹é¢¨æ¨¡å¼
    var activeMicrophoneMode: AVCaptureDevice.MicrophoneMode {
        AVCaptureDevice.activeMicrophoneMode
    }

    // MARK: - Permission

    func requestPermission() async -> Bool {
        await withCheckedContinuation { continuation in
            AVAudioApplication.requestRecordPermission { granted in
                continuation.resume(returning: granted)
            }
        }
    }

    // MARK: - Recording Methods

    /// é–‹å§‹éŒ„éŸ³
    func startRecording() throws {
        guard recordingState != .recording else { return }

        // é…ç½®éŸ³é »æœƒè©±
        let rtcAudioSession = RTCAudioSession.sharedInstance()
        rtcAudioSession.lockForConfiguration()
        do {
            try rtcAudioSession.setCategory(AVAudioSession.Category.playAndRecord)
            try rtcAudioSession.setMode(AVAudioSession.Mode.voiceChat)
            try rtcAudioSession.setActive(true)
        } catch {
            print("âŒ [WebRTC] éŸ³é »æœƒè©±é…ç½®å¤±æ•—: \(error)")
        }
        rtcAudioSession.unlockForConfiguration()

        updateOutputRoute()

        // å‰µå»º WebRTC éŸ³é »è»Œé“
        let audioConstraints = RTCMediaConstraints(
            mandatoryConstraints: nil,
            optionalConstraints: [
                "googEchoCancellation": "true",
                "googAutoGainControl": "false",
                "googNoiseSuppression": "true",
                "googHighpassFilter": "true"
            ]
        )

        audioSource = factory.audioSource(with: audioConstraints)
        guard let source = audioSource else {
            throw WebRTCRecordingError.webrtcInitFailed
        }

        localAudioTrack = factory.audioTrack(with: source, trackId: "audio0")
        localAudioTrack?.isEnabled = true

        print("âœ… [WebRTC] éŸ³é »è»Œé“å·²å‰µå»º")
        print("   AEC: å•Ÿç”¨")

        // åˆå§‹åŒ–éŒ„éŸ³ï¼ˆé€™æœƒè§¸ç™¼ delegate å›èª¿ï¼‰
        let result = audioDeviceModule.initRecording()
        if result != 0 {
            print("âš ï¸ [WebRTC] initRecording è¿”å›: \(result)")
        }

        // é–‹å§‹éŒ„éŸ³
        let startResult = audioDeviceModule.startRecording()
        if startResult != 0 {
            print("âš ï¸ [WebRTC] startRecording è¿”å›: \(startResult)")
        }

        // å•Ÿå‹•ç·©è¡å€å®šæ™‚å™¨
        startBufferTimer()

        recordingState = .recording
        print("ğŸ™ï¸ [WebRTC] é–‹å§‹éŒ„éŸ³ï¼ˆAudioEngine æ¨¡å¼ï¼‰")
    }

    /// åœæ­¢éŒ„éŸ³
    func stopRecording() {
        guard recordingState == .recording else { return }

        stopBufferTimer()
        flushBuffer()

        // ç§»é™¤ tap
        tapMixerNode?.removeTap(onBus: 0)
        tapMixerNode = nil

        // åœæ­¢ WebRTC éŒ„éŸ³
        audioDeviceModule.stopRecording()

        // åœæ­¢ TTS
        stopTTS()

        // åœæ­¢éŸ³é »è»Œé“
        localAudioTrack?.isEnabled = false
        localAudioTrack = nil
        audioSource = nil

        print("â¹ï¸ [WebRTC] åœæ­¢éŒ„éŸ³ (ç¸½è¨ˆç™¼é€ \(sendCount) æ¬¡)")
        sendCount = 0
        recordingState = .idle
        isManualSendingPaused = true
    }

    // MARK: - Audio Processing

    /// è™•ç†å¾ tap æ¥æ”¶çš„éŸ³é »æ•¸æ“š
    private func processAudioBuffer(_ buffer: AVAudioPCMBuffer) {
        guard let data = convertToWebSocketFormat(buffer) else { return }
        audioBufferCollector.append(data)
    }

    /// è½‰æ›éŸ³é »æ ¼å¼
    private func convertToWebSocketFormat(_ buffer: AVAudioPCMBuffer) -> Data? {
        guard let outFormat = outputFormat else { return nil }

        let inputFormat = buffer.format

        // å¦‚æœå·²ç¶“æ˜¯ç›®æ¨™æ ¼å¼
        if inputFormat.sampleRate == 16000 &&
           inputFormat.channelCount == 1 &&
           inputFormat.commonFormat == .pcmFormatInt16 {
            if let channelData = buffer.int16ChannelData {
                let frameLength = Int(buffer.frameLength)
                return Data(bytes: channelData[0], count: frameLength * 2)
            }
        }

        // éœ€è¦è½‰æ›
        if audioConverter == nil || audioConverter?.inputFormat != inputFormat {
            audioConverter = AVAudioConverter(from: inputFormat, to: outFormat)
        }

        guard let converter = audioConverter else { return nil }

        let ratio = outFormat.sampleRate / inputFormat.sampleRate
        let outputFrameCapacity = AVAudioFrameCount(Double(buffer.frameLength) * ratio) + 1

        guard let outputBuffer = AVAudioPCMBuffer(
            pcmFormat: outFormat,
            frameCapacity: outputFrameCapacity
        ) else { return nil }

        var hasProvidedData = false
        var error: NSError?

        converter.convert(to: outputBuffer, error: &error) { _, outStatus in
            if hasProvidedData {
                outStatus.pointee = .noDataNow
                return nil
            }
            hasProvidedData = true
            outStatus.pointee = .haveData
            return buffer
        }

        if error != nil { return nil }

        if let channelData = outputBuffer.int16ChannelData {
            let frameLength = Int(outputBuffer.frameLength)
            if frameLength > 0 {
                return Data(bytes: channelData[0], count: frameLength * 2)
            }
        }

        return nil
    }

    // MARK: - Push-to-Talk

    func startSending() {
        isManualSendingPaused = false
        print("ğŸ™ï¸ [WebRTC] é–‹å§‹ç™¼é€éŸ³é »")

        if !audioBufferCollector.isEmpty {
            print("ğŸ“¦ [WebRTC] ç«‹å³ç™¼é€ç·©è¡: \(audioBufferCollector.count) å€‹ç‰‡æ®µ")
            flushBuffer()
        }
    }

    func stopSending() {
        flushRemainingAudio()
        sendTrailingSilence()
        onEndUtterance?()
        isManualSendingPaused = true
        print("â¸ï¸ [WebRTC] åœæ­¢ç™¼é€éŸ³é »")
    }

    private func flushRemainingAudio() {
        guard !audioBufferCollector.isEmpty else { return }

        var combinedData = Data()
        for buffer in audioBufferCollector {
            combinedData.append(buffer)
        }
        audioBufferCollector.removeAll()

        if combinedData.isEmpty { return }

        var offset = 0
        while offset < combinedData.count {
            let chunkSize = min(maxChunkSize, combinedData.count - offset)
            let chunk = combinedData.subdata(in: offset..<(offset + chunkSize))
            sendCount += 1
            audioDataSubject.send(chunk)
            offset += chunkSize
        }
    }

    private func sendTrailingSilence() {
        let bytesPerChunk = 8000
        for _ in 0..<4 {
            let silenceData = Data(count: bytesPerChunk)
            sendCount += 1
            audioDataSubject.send(silenceData)
        }
        print("ğŸ”‡ [WebRTC] ç™¼é€å°¾éƒ¨éœéŸ³")
    }

    // MARK: - TTS Playback

    /// æ’­æ”¾ TTS éŸ³é »ï¼ˆé€šé WebRTC Engine æ’­æ”¾ï¼ŒAEC è‡ªå‹•è™•ç†å›éŸ³ï¼‰
    func playTTS(audioData: Data, text: String? = nil) throws {
        stopTTS()

        guard let engine = webrtcEngine else {
            print("âŒ [WebRTC] Engine æœªæº–å‚™å¥½ï¼Œç„¡æ³•æ’­æ”¾ TTS")
            throw WebRTCTTSError.engineNotReady
        }

        currentTTSText = text
        isPlayingTTS = true
        hasTriggeredCompletion = false

        // å‰µå»ºæ’­æ”¾ç¯€é»ï¼ˆå¦‚æœé‚„æ²’æœ‰ï¼‰
        if ttsPlayerNode == nil {
            ttsPlayerNode = AVAudioPlayerNode()
            ttsEQNode = AVAudioUnitEQ(numberOfBands: 3)
        }

        guard let player = ttsPlayerNode, let eq = ttsEQNode else {
            throw WebRTCTTSError.playbackFailed
        }

        // é€£æ¥ç¯€é»åˆ° WebRTC Engineï¼ˆå¦‚æœé‚„æ²’é€£æ¥ï¼‰
        if !ttsNodesConnected {
            engine.attach(player)
            engine.attach(eq)

            let format = engine.mainMixerNode.outputFormat(forBus: 0)
            engine.connect(player, to: eq, format: format)
            engine.connect(eq, to: engine.mainMixerNode, format: format)

            ttsNodesConnected = true
            updateVolumeGain()
            print("âœ… [WebRTC] TTS ç¯€é»å·²é€£æ¥åˆ° WebRTC Engine")
        }

        // å¯«å…¥è‡¨æ™‚æ–‡ä»¶
        let tempURL = FileManager.default.temporaryDirectory.appendingPathComponent(UUID().uuidString + ".mp3")
        try audioData.write(to: tempURL)

        ttsAudioFile = try AVAudioFile(forReading: tempURL)

        guard let audioFile = ttsAudioFile else {
            throw WebRTCTTSError.audioFileError
        }

        print("ğŸ”Š [WebRTC] TTS æ’­æ”¾ä¸­ï¼ˆå…¨é›™å·¥ï¼ŒAEC è™•ç†å›éŸ³ï¼‰")
        print("   æ–‡æœ¬: \(text?.prefix(30) ?? "unknown")...")
        print("   å¢ç›Š: +\(Int(volumeBoostDB)) dB")

        player.scheduleFile(audioFile, at: nil, completionCallbackType: .dataPlayedBack) { [weak self] _ in
            DispatchQueue.main.async {
                self?.onTTSPlaybackComplete(tempURL: tempURL)
            }
        }

        player.play()
        startPlaybackMonitor()
    }

    private func onTTSPlaybackComplete(tempURL: URL) {
        guard !hasTriggeredCompletion else { return }
        hasTriggeredCompletion = true

        print("âœ… [WebRTC] TTS æ’­æ”¾å®Œæˆ")
        isPlayingTTS = false
        currentTTSText = nil

        playbackTimer?.invalidate()
        playbackTimer = nil

        try? FileManager.default.removeItem(at: tempURL)
        onTTSPlaybackFinished?()
    }

    func stopTTS() {
        playbackTimer?.invalidate()
        playbackTimer = nil
        ttsPlayerNode?.stop()

        if let audioFile = ttsAudioFile {
            try? FileManager.default.removeItem(at: audioFile.url)
        }
        ttsAudioFile = nil

        isPlayingTTS = false
        currentTTSText = nil
    }

    private func startPlaybackMonitor() {
        playbackTimer?.invalidate()
        playbackTimer = Timer.scheduledTimer(withTimeInterval: 0.1, repeats: true) { [weak self] timer in
            guard let self = self,
                  let player = self.ttsPlayerNode else {
                timer.invalidate()
                return
            }

            if !player.isPlaying && self.isPlayingTTS {
                timer.invalidate()
                if let url = self.ttsAudioFile?.url {
                    self.onTTSPlaybackComplete(tempURL: url)
                }
            }
        }
    }

    /// æ›´æ–°éŸ³é‡å¢ç›Š
    private func updateVolumeGain() {
        guard let eq = ttsEQNode else { return }

        let perBandGain = volumeBoostDB / 3.0

        eq.bands[0].filterType = .lowShelf
        eq.bands[0].frequency = 250
        eq.bands[0].gain = perBandGain
        eq.bands[0].bypass = false

        eq.bands[1].filterType = .parametric
        eq.bands[1].frequency = 1000
        eq.bands[1].bandwidth = 1.0
        eq.bands[1].gain = perBandGain
        eq.bands[1].bypass = false

        eq.bands[2].filterType = .highShelf
        eq.bands[2].frequency = 4000
        eq.bands[2].gain = perBandGain
        eq.bands[2].bypass = false

        print("ğŸ”Š [WebRTC] éŸ³é‡å¢ç›Š: +\(Int(volumeBoostDB)) dB")
    }

    // MARK: - Buffer Management

    private func startBufferTimer() {
        bufferTimer = Timer.scheduledTimer(withTimeInterval: bufferInterval, repeats: true) { [weak self] _ in
            self?.flushBuffer()
        }
    }

    private func stopBufferTimer() {
        bufferTimer?.invalidate()
        bufferTimer = nil
    }

    private func flushBuffer() {
        guard !audioBufferCollector.isEmpty else { return }

        if isManualSendingPaused {
            while audioBufferCollector.count > 4 {
                audioBufferCollector.removeFirst()
            }
            return
        }

        var combinedData = Data()
        for buffer in audioBufferCollector {
            combinedData.append(buffer)
        }
        audioBufferCollector.removeAll()

        var offset = 0
        while offset < combinedData.count {
            let chunkSize = min(maxChunkSize, combinedData.count - offset)
            let chunk = combinedData.subdata(in: offset..<(offset + chunkSize))

            sendCount += 1
            if sendCount == 1 || sendCount % 20 == 0 {
                print("ğŸ“¤ [WebRTC] ç™¼é€éŸ³é » #\(sendCount): \(chunk.count) bytes")
            }
            audioDataSubject.send(chunk)

            offset += chunkSize
        }
    }
}

// MARK: - RTCAudioDeviceModuleDelegate

extension WebRTCAudioManager: RTCAudioDeviceModuleDelegate {

    func audioDeviceModule(_ audioDeviceModule: RTCAudioDeviceModule,
                          didReceiveSpeechActivityEvent speechActivityEvent: RTCSpeechActivityEvent) {
        switch speechActivityEvent {
        case .started:
            print("ğŸ¤ [WebRTC] èªéŸ³æ´»å‹•é–‹å§‹")
        case .ended:
            print("ğŸ”‡ [WebRTC] èªéŸ³æ´»å‹•çµæŸ")
        @unknown default:
            break
        }
    }

    func audioDeviceModule(_ audioDeviceModule: RTCAudioDeviceModule,
                          didCreateEngine engine: AVAudioEngine) -> Int {
        print("âœ… [WebRTC Delegate] AVAudioEngine å·²å‰µå»º")
        self.webrtcEngine = engine
        return 0
    }

    func audioDeviceModule(_ audioDeviceModule: RTCAudioDeviceModule,
                          willEnableEngine engine: AVAudioEngine,
                          isPlayoutEnabled: Bool,
                          isRecordingEnabled: Bool) -> Int {
        print("ğŸ”§ [WebRTC Delegate] Engine å³å°‡å•Ÿç”¨")
        print("   Playout: \(isPlayoutEnabled), Recording: \(isRecordingEnabled)")
        return 0
    }

    func audioDeviceModule(_ audioDeviceModule: RTCAudioDeviceModule,
                          willStartEngine engine: AVAudioEngine,
                          isPlayoutEnabled: Bool,
                          isRecordingEnabled: Bool) -> Int {
        print("â–¶ï¸ [WebRTC Delegate] Engine å³å°‡å•Ÿå‹•")
        return 0
    }

    func audioDeviceModule(_ audioDeviceModule: RTCAudioDeviceModule,
                          didStopEngine engine: AVAudioEngine,
                          isPlayoutEnabled: Bool,
                          isRecordingEnabled: Bool) -> Int {
        print("â¹ï¸ [WebRTC Delegate] Engine å·²åœæ­¢")
        return 0
    }

    func audioDeviceModule(_ audioDeviceModule: RTCAudioDeviceModule,
                          didDisableEngine engine: AVAudioEngine,
                          isPlayoutEnabled: Bool,
                          isRecordingEnabled: Bool) -> Int {
        print("ğŸ”‡ [WebRTC Delegate] Engine å·²ç¦ç”¨")
        ttsNodesConnected = false
        return 0
    }

    func audioDeviceModule(_ audioDeviceModule: RTCAudioDeviceModule,
                          willReleaseEngine engine: AVAudioEngine) -> Int {
        print("ğŸ—‘ï¸ [WebRTC Delegate] Engine å³å°‡é‡‹æ”¾")
        self.webrtcEngine = nil
        ttsNodesConnected = false
        return 0
    }

    /// â­ï¸ é—œéµï¼šé…ç½®è¼¸å…¥è·¯å¾‘ - åœ¨é€™è£¡å®‰è£ tap æ•ç²éŸ³é »
    func audioDeviceModule(_ audioDeviceModule: RTCAudioDeviceModule,
                          engine: AVAudioEngine,
                          configureInputFromSource source: AVAudioNode?,
                          toDestination destination: AVAudioNode,
                          format: AVAudioFormat,
                          context: [AnyHashable: Any]) -> Int {
        print("ğŸ¤ [WebRTC Delegate] é…ç½®è¼¸å…¥è·¯å¾‘")
        print("   Source: \(source != nil ? "inputNode" : "nil")")
        print("   Format: \(format.sampleRate)Hz, \(format.channelCount)ch")

        guard let inputSource = source else {
            print("âš ï¸ [WebRTC Delegate] Source ç‚º nilï¼Œç„¡æ³•å®‰è£ tap")
            return 0
        }

        // â­ï¸ å•Ÿç”¨ Voice Processingï¼ˆæ”¯æ´ç³»çµ± Voice Isolationï¼‰
        let inputNode = engine.inputNode
        do {
            try inputNode.setVoiceProcessingEnabled(true)
            inputNode.isVoiceProcessingAGCEnabled = true
            inputNode.isVoiceProcessingBypassed = false
            print("âœ… [WebRTC Delegate] Voice Processing å·²å•Ÿç”¨ï¼ˆæ”¯æ´ Voice Isolationï¼‰")
        } catch {
            print("âš ï¸ [WebRTC Delegate] Voice Processing å•Ÿç”¨å¤±æ•—: \(error)")
        }

        // å‰µå»º Mixer ç¯€é»ç”¨æ–¼ tap
        let mixer = AVAudioMixerNode()
        engine.attach(mixer)

        // é€£æ¥ï¼šsource â†’ mixer â†’ destination
        engine.connect(inputSource, to: mixer, format: format)
        engine.connect(mixer, to: destination, format: format)

        // åœ¨ mixer ä¸Šå®‰è£ tap
        mixer.installTap(onBus: 0, bufferSize: 4096, format: format) { [weak self] buffer, _ in
            self?.processAudioBuffer(buffer)
        }

        self.tapMixerNode = mixer
        print("âœ… [WebRTC Delegate] Tap å·²å®‰è£åˆ°è¼¸å…¥è·¯å¾‘")

        return 0
    }

    /// é…ç½®è¼¸å‡ºè·¯å¾‘
    func audioDeviceModule(_ audioDeviceModule: RTCAudioDeviceModule,
                          engine: AVAudioEngine,
                          configureOutputFromSource source: AVAudioNode,
                          toDestination destination: AVAudioNode?,
                          format: AVAudioFormat,
                          context: [AnyHashable: Any]) -> Int {
        print("ğŸ”Š [WebRTC Delegate] é…ç½®è¼¸å‡ºè·¯å¾‘")
        print("   Format: \(format.sampleRate)Hz, \(format.channelCount)ch")
        return 0
    }

    func audioDeviceModuleDidUpdateDevices(_ audioDeviceModule: RTCAudioDeviceModule) {
        print("ğŸ”„ [WebRTC Delegate] è¨­å‚™åˆ—è¡¨å·²æ›´æ–°")
    }
}
